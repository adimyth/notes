<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How to win a data science competition (Week 4) | Notes</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="How to win a data science competition (Week 4)" />
<meta name="author" content="Aditya Mishra" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes from Week 4 of Coursera’s ‘How to win a data science competition’. Instructors in this week, detail about different hyperparameter tuning libraries, HyperOpt in particular. Also, they discuss about regular hyperparameters in XGBoost &amp; RF." />
<meta property="og:description" content="Notes from Week 4 of Coursera’s ‘How to win a data science competition’. Instructors in this week, detail about different hyperparameter tuning libraries, HyperOpt in particular. Also, they discuss about regular hyperparameters in XGBoost &amp; RF." />
<link rel="canonical" href="/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html" />
<meta property="og:url" content="/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html" />
<meta property="og:site_name" content="Notes" />
<meta property="og:image" content="/notes/images/ackley.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-29T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Aditya Mishra"},"description":"Notes from Week 4 of Coursera’s ‘How to win a data science competition’. Instructors in this week, detail about different hyperparameter tuning libraries, HyperOpt in particular. Also, they discuss about regular hyperparameters in XGBoost &amp; RF.","@type":"BlogPosting","headline":"How to win a data science competition (Week 4)","dateModified":"2019-06-29T00:00:00-05:00","datePublished":"2019-06-29T00:00:00-05:00","image":"/notes/images/ackley.gif","url":"/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html","mainEntityOfPage":{"@type":"WebPage","@id":"/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/notes/feed.xml" title="Notes" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-140233777-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How to win a data science competition (Week 4) | Notes</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="How to win a data science competition (Week 4)" />
<meta name="author" content="Aditya Mishra" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes from Week 4 of Coursera’s ‘How to win a data science competition’. Instructors in this week, detail about different hyperparameter tuning libraries, HyperOpt in particular. Also, they discuss about regular hyperparameters in XGBoost &amp; RF." />
<meta property="og:description" content="Notes from Week 4 of Coursera’s ‘How to win a data science competition’. Instructors in this week, detail about different hyperparameter tuning libraries, HyperOpt in particular. Also, they discuss about regular hyperparameters in XGBoost &amp; RF." />
<link rel="canonical" href="/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html" />
<meta property="og:url" content="/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html" />
<meta property="og:site_name" content="Notes" />
<meta property="og:image" content="/notes/images/ackley.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-29T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Aditya Mishra"},"description":"Notes from Week 4 of Coursera’s ‘How to win a data science competition’. Instructors in this week, detail about different hyperparameter tuning libraries, HyperOpt in particular. Also, they discuss about regular hyperparameters in XGBoost &amp; RF.","@type":"BlogPosting","headline":"How to win a data science competition (Week 4)","dateModified":"2019-06-29T00:00:00-05:00","datePublished":"2019-06-29T00:00:00-05:00","image":"/notes/images/ackley.gif","url":"/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html","mainEntityOfPage":{"@type":"WebPage","@id":"/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="/notes/feed.xml" title="Notes" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-140233777-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">Notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/_pages/about.html">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to win a data science competition (Week 4)</h1><p class="page-description">Notes from Week 4 of Coursera's 'How to win a data science competition'. Instructors in this week, detail about different hyperparameter tuning libraries, HyperOpt in particular. Also, they discuss about regular hyperparameters in XGBoost & RF.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-06-29T00:00:00-05:00" itemprop="datePublished">
        Jun 29, 2019
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Aditya Mishra</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#hyperparameter tuning">hyperparameter tuning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#datascience">datascience</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#kaggle">kaggle</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="how-to-win-a-data-science-competition-week-4">How to win a data science competition (Week 4)</h1>

<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<ol>
  <li>Select the most important parameters</li>
  <li>Understand how exactly they influence the training</li>
</ol>

<p>Libraries -</p>

<ul>
  <li>HyperOpt</li>
  <li>Scikit-optimize</li>
  <li>GPyOpt</li>
  <li>RoBo</li>
</ul>

<h3 id="hyperopt">HyperOpt</h3>
<ul>
  <li><a href="https://www.kaggle.com/fanvacoolt/tutorial-on-hyperopt">Kaggle HyperOpt Tutorial</a></li>
  <li><a href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">Medium Blog</a></li>
  <li><a href="https://medium.com/vantageai/bringing-back-the-time-spent-on-hyperparameter-tuning-with-bayesian-optimisation-2e21a3198afb">Working Example</a></li>
  <li><a href="https://districtdatalabs.silvrback.com/parameter-tuning-with-hyperopt">All Classifiers Example</a></li>
</ul>

<p>Bayesian optimisation takes into account past evaluations when choosing the hyperparameter set to evaluate next. By choosing its parameter combinations in an informed way, it enables itself to focus on those areas of the parameter space that it believes will bring the most promising validation scores.</p>

<h4 id="search-space">Search Space</h4>
<p>Bayesian Optimisation operates along probability distributions for each parameter that it will sample from. We specify a probability distribution for the hyperparameters to sample from. One could choose from the following -</p>

<ul>
  <li><code class="highlighter-rouge">hp.choice</code></li>
  <li><code class="highlighter-rouge">hp.uniform</code></li>
  <li><code class="highlighter-rouge">hp.normal</code></li>
  <li><code class="highlighter-rouge">hp.lognormal</code></li>
</ul>

<h4 id="objective-function">Objective Function</h4>
<p>The objective function takes in hyperparameters and outputs a single real-valued score that we want to <em>minimize</em> (or maximize). It simply takes in a set of hyperparameters and outputs a score that indicates how well a set of hyperparameters performs on the validation set. For example - RMSE</p>

<p>The entire concept of Bayesian model-based optimization is to <em>reduce the number of times the objective function needs to be run by choosing only the most promising set of hyperparameters</em> to evaluate based on previous calls to the evaluation function.</p>

<blockquote>
  <p>Objective Function is usually the loss function. By default <code class="highlighter-rouge">hyperopt</code> tries to minimize the objective function.</p>
</blockquote>

<h4 id="surrogate-function">Surrogate Function</h4>
<p>Surrogate Function is an approximation of objective function. It is used to propose parameter sets to the objective function that likely yield an improvement. HyperOpt uses something called <em>Tree Parzen Estimator (TPE).</em></p>

<h4 id="selection-function">Selection Function</h4>
<p>The selection function is the criteria by which the next set of hyperparameters are chosen from the surrogate function. The most common choice of criteria is <strong>Expected Improvement</strong>.</p>

<h3 id="xgboost">XGBoost</h3>
<h4 id="increasing-models-capacity">Increasing Model’s Capacity</h4>
<ul>
  <li><code class="highlighter-rouge">max_depth</code> - Increasing this value will make the model more complex and more likely to overfit. Usually start in between [3, 7]. Increase only if there’s an improvement in score.</li>
  <li><code class="highlighter-rouge">subsample</code> - Controls the fraction of objects to use when fitting a true. Ranges between [0-1]. Has some sort of regularisation effect and prone to overfitting.</li>
  <li><code class="highlighter-rouge">colsample_bytree</code> - Controls the fraction of features to consider at every split. [0-1]</li>
  <li><code class="highlighter-rouge">eta</code> - Learning rate</li>
  <li><code class="highlighter-rouge">num_round</code> - Number of trees to build.</li>
</ul>

<h4 id="decreasing-models-capacity">Decreasing Model’s Capacity</h4>
<ul>
  <li><code class="highlighter-rouge">min_child_weight</code> - The larger <code class="highlighter-rouge">min_child_weight</code> is, the more conservative the algorithm will be. Ranges $[0, _{inf}]$</li>
  <li><code class="highlighter-rouge">gamma</code> - Minimum loss required to make a further split. Larger values results in conservative model. Ranges $[0, _{inf}]$</li>
  <li><code class="highlighter-rouge">lambda</code> - L2 regularization.</li>
  <li><code class="highlighter-rouge">alpha</code> - L1 regularization</li>
</ul>

<p>Usually after finding the appropriated values for <code class="highlighter-rouge">num_round</code> and <code class="highlighter-rouge">eta</code>. Divide <code class="highlighter-rouge">eta</code> by <code class="highlighter-rouge">alpha</code> &amp; multiply <code class="highlighter-rouge">num_round</code> by <code class="highlighter-rouge">alpha</code>, this will usually give a slightly better result.</p>

<h3 id="randomforestextratrees">RandomForest/ExtraTrees</h3>
<p>Unlike in Gradient Boosting Trees &amp; it’s variants, RF builds trees <em>parallely</em> &amp; hence increasing the number of trees won’t overfit the model.</p>
<ul>
  <li><code class="highlighter-rouge">n_estimators</code> - Start from small values say 10 &amp; check the time required to fit these 10 trees. If its not too high then use a larger value.</li>
  <li><code class="highlighter-rouge">max_depth</code> - Usually tree depth for RF is greater then GBDTs.</li>
  <li><code class="highlighter-rouge">max_features</code></li>
  <li><code class="highlighter-rouge">min_samples_leaf</code></li>
</ul>

<h2 id="neural-networks">Neural Networks</h2>
<h3 id="increasing-models-capacity-1">Increasing Model’s Capacity</h3>
<ul>
  <li>Number of neurons per layer</li>
  <li>Number of layers</li>
  <li>Adaptive Methods (Adam, Adagrad) - Leads to overfitting sometimes</li>
  <li>Batch Size - Larger batch size causes overfitting</li>
</ul>

<h3 id="decreasing-models-capacity-1">Decreasing Model’s Capacity</h3>
<ul>
  <li>SGD+momentum - Even though it leads to slower learning. The model generalises better.</li>
  <li>Regularization - L2/L1, Dropout/Dropconnect,</li>
</ul>

<blockquote>
  <p>Some people follow the rule - If you increase your batch size by a factor of <code class="highlighter-rouge">alpha</code>, increase your learning rate by the same factor as well.</p>
</blockquote>

<h3 id="cross-validation-strategy">Cross Validation Strategy</h3>
<p>Creating a validation strategy, is to create a validation approach that resembles what you are being tested on. The validation data should be <strong>consistent</strong> with the test data.</p>

<h4 id="time-based">Time Based</h4>
<p>Always have past data predicting the future data. Also the intervals need to be similar with test data. So, if test data is 3 months in the future, the validation data should also be 3 months future compared to the training data.</p>

<h4 id="stratified">Stratified</h4>
<p>Different entities than train?  Suppose the test data has entities which are not present in the training data, then the validation dataset should be created such that there are entities in it which are not present in the train data.</p>

<h3 id="ensembling">Ensembling</h3>
<ol>
  <li>Smaller data requires simpler ensembling techniques like averaging.</li>
  <li>Bigger data works well with Stacking like techniques</li>
</ol>

<h2 id="statistic-and-distance-based-feature-engineering">Statistic and Distance based feature engineering</h2>
<ul>
  <li>Calculating various statistics of one feature grouped by another</li>
  <li>Features derived from neighbourhood analysis of a given point</li>
</ul>

<h2 id="matrix-factorizations">Matrix Factorizations</h2>
<p><a href="https://www.coursera.org/learn/competitive-data-science/lecture/8o1Hc/matrix-factorizations">Coursera Matrix Factorization</a></p>

<h3 id="general-notes">General Notes</h3>
<ul>
  <li>MF is a very general approach for dimensionality reduction and feature selection</li>
  <li>It can be applied for transforming categorical features into real-valued features</li>
</ul>

<h3 id="implementation-notes">Implementation Notes</h3>
<ul>
  <li>Matrix factorisation can be applied to only a subset of columns if desired</li>
  <li>We can use matrix factorisation to get another representation of the same data, especially useful for ensemble models</li>
  <li>Loss of information</li>
  <li>The number of latent factors to use <strong>must be treated as a hyperparamater &amp; must be tuned.</strong></li>
</ul>

<h3 id="nmf-non-negative-matrix-factorisation">NMF (Non-negative Matrix Factorisation)</h3>
<p>It transforms data in a way that makes it more suitable for decision trees. NMF cannot be applied on negative values. “Standardized” means that every feature column has zero mean and unit variance. This implies that we have negative values and cannot apply NMF.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">conctenate</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">])</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span>
<span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p>Manifold learning in general are non-linear methods of dimensionality reduction. Apply t-SNE to concatenation of train and test and split projection back.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="adimyth/notes"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Aditya Mishra</li>
          <li><a class="u-email" href="mailto:mishraaditya6991@gmail.com">mishraaditya6991@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Documenting all my learning notes here. Contains notes from different MOOCs, Blogs &amp; Other YouTube Videos.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/adimyth" title="adimyth"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/adi_myth" title="adi_myth"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
