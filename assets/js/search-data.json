{
  
    
        "post0": {
            "title": "Word Level Seq2Seq Model",
            "content": ". Word Level Seq2Seq Model . Sequence-to-sequence Neural Machine Translation is an example of Conditional Language Model. . Language Model - Decoder is predicting the next word of the target sentence based on the sequence generated so far | Conditional - The predictions are conditioned on the source sentence x and the generated target seq | . It calculate $P(y|x)$ where $x$ is the source sentence &amp; $y$ is the target sentence. $$P(y|x)=P(y_{1}|x)P(y_{2}|y_{1},x)P(y_{3}|y_{1},y_{2},x)...P(y_{T}|y_{1},...,y_{T-1},x)$$ . Any of the above term can be interpreted as probability of next word, given target words so far and source sentence x . Encoder . Decoder . Stepwise, the decoder operates as - . . Dataset . English to Spanish Conversion - http://www.manythings.org/anki/spa-eng.zip . !wget http://www.manythings.org/anki/spa-eng.zip . --2020-04-20 06:06:41-- http://www.manythings.org/anki/spa-eng.zip Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:3033::6818:6dc4, ... Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 4781548 (4.6M) [application/zip] Saving to: ‘spa-eng.zip’ spa-eng.zip 100%[===================&gt;] 4.56M 3.06MB/s in 1.5s 2020-04-20 06:06:43 (3.06 MB/s) - ‘spa-eng.zip’ saved [4781548/4781548] . !unzip -l spa-eng.zip !unzip spa-eng.zip . Archive: spa-eng.zip Length Date Time Name - -- - 1441 2020-03-15 02:17 _about.txt 18493172 2020-03-15 02:17 spa.txt - 18494613 2 files Archive: spa-eng.zip inflating: _about.txt inflating: spa.txt . from collections import Counter import matplotlib.pyplot as plt from itertools import islice import math import numpy as np import pandas as pd import random import re import requests import seaborn as sns import string from string import digits from sklearn.utils import shuffle from sklearn.model_selection import train_test_split import tensorflow as tf from tensorflow.keras.callbacks import CSVLogger, EarlyStopping from tensorflow.keras.layers import Input, LSTM, Embedding, Dense from tensorflow.keras.models import Model, load_model from tensorflow.keras.utils import plot_model . from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution disable_eager_execution() . . Note: Disabling eager execution because all zeros mask raises some CuDNN kernel level issue. Refer here . %matplotlib inline sns.set_style(&quot;whitegrid&quot;) . lines = pd.read_table(&#39;spa.txt&#39;, names=[&#39;english&#39;, &#39;spanish&#39;, &#39;attributes&#39;]) # lines = pd.DataFrame({&quot;english&quot;: [&quot;Juan eats apples&quot;], &quot;spanish&quot;: [&quot;Juan come manzanas&quot;], &quot;attributes&quot;: &quot;&quot;}) . lines.shape . (123770, 3) . lines = lines.drop(columns=[&#39;attributes&#39;]) . for col in lines.columns: # lowercase lines[col] = lines[col].apply(lambda x: x.lower()) # remove quotes lines[col] = lines[col].apply(lambda x: re.sub(&quot;&#39;&quot;, &quot;&quot;, x)) # remove punctuations lines[col] = lines[col].apply(lambda x: &#39;&#39;.join(ch for ch in x if ch not in set(string.punctuation))) # remove numbers remove_digits = str.maketrans(&#39;&#39;, &#39;&#39;, digits) lines[col] = lines[col].apply(lambda x: x.translate(remove_digits)) # remove unnecessary spaces lines[col] = lines[col].apply(lambda x: x.strip()) lines[col] = lines[col].apply(lambda x: re.sub(&quot; +&quot;, &quot; &quot;, x)) . # Add start and end tokens to target sequences lines[&#39;spanish&#39;] = lines[&#39;spanish&#39;].apply(lambda x : &#39;START_ &#39;+ x + &#39; _END&#39;) . pd.set_option(&#39;display.max_colwidth&#39;, 100) . lines.head(10) . english spanish . 0 go | START_ ve _END | . 1 go | START_ vete _END | . 2 go | START_ vaya _END | . 3 go | START_ váyase _END | . 4 hi | START_ hola _END | . 5 run | START_ ¡corre _END | . 6 run | START_ ¡corran _END | . 7 run | START_ ¡corra _END | . 8 run | START_ ¡corred _END | . 9 run | START_ corred _END | . Creating Vocabulary . Create vocabulary of english and spanish words . # English Vocab all_eng_words = set() for eng in lines[&#39;english&#39;]: for word in eng.split(): if word not in all_eng_words: all_eng_words.add(word) print(f&quot;English Vocab: {len(all_eng_words)}&quot;) . English Vocab: 13475 . # Spanish Vocab all_spa_words = set() for spa in lines[&#39;spanish&#39;]: for word in spa.split(): if word not in all_spa_words: all_spa_words.add(word) print(f&quot;Spanish Vocab: {len(all_spa_words)}&quot;) . Spanish Vocab: 27264 . # Max Length of source sequence lenght_list_eng=[] for l in lines[&#39;english&#39;]: lenght_list_eng.append(len(l.split(&#39; &#39;))) max_length_src = np.max(lenght_list_eng) print(f&quot;Max Length Sentence (English): {max_length_src}&quot;) . Max Length Sentence (English): 47 . # Max Length of target sequence lenght_list_spa=[] for l in lines[&#39;spanish&#39;]: lenght_list_spa.append(len(l.split(&#39; &#39;))) max_length_tar = np.max(lenght_list_spa) print(f&quot;Max Length Sentence (Spanish): {max_length_src}&quot;) . Max Length Sentence (Spanish): 47 . matches = [i for i, j in zip(lenght_list_eng, lenght_list_spa) if i == j] print(f&quot;Number of matches: {len(matches)} ({(len(matches)*100/lines.shape[0]):.2f})&quot;) . Number of matches: 13865 (11.20) . lines.head() . english spanish . 0 go | START_ ve _END | . 1 go | START_ vete _END | . 2 go | START_ vaya _END | . 3 go | START_ váyase _END | . 4 hi | START_ hola _END | . input_words = sorted(list(all_eng_words)) target_words = sorted(list(all_spa_words)) num_encoder_tokens = len(all_eng_words) num_decoder_tokens = len(all_spa_words) num_encoder_tokens, num_decoder_tokens . (13475, 27264) . num_encoder_tokens += 1 # For zero padding num_decoder_tokens += 1 # For zero padding . Tokenization . def take(n, iterable): &quot;Return first n items of the iterable as a list&quot; return list(islice(iterable, n)) . input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)]) target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)]) . n_items = take(10, input_token_index.items()) for k,v in n_items: print(k, v) . a 1 aardvark 2 aardvarks 3 aaron 4 aback 5 abandon 6 abandoned 7 abandoning 8 abate 9 abated 10 . n_items = take(10, target_token_index.items()) for k,v in n_items: print(k, v) . START_ 1 _END 2 a 3 aabe 4 aah 5 aaron 6 abajo 7 abandona 8 abandonada 9 abandonadas 10 . reverse_input_char_index = dict((i, word) for word, i in input_token_index.items()) reverse_target_char_index = dict((i, word) for word, i in target_token_index.items()) . lines = shuffle(lines) lines.head(10) . english spanish . 36217 you are my best friend | START_ eres mi mejor amigo _END | . 118304 if you do not have this program you can download it now | START_ si usted no dispone de este programa puede descargarlo ahora _END | . 47672 she got married last year | START_ se casó el año pasado _END | . 32053 can you climb the tree | START_ ¿puedes trepar al árbol _END | . 34632 someone stole my money | START_ alguien se voló mi plata _END | . 93415 we have till tomorrow night to decide | START_ tenemos hasta mañana por la noche para decidirnos _END | . 103998 tom claimed he killed mary in selfdefense | START_ tom alegó que mató a mary en defensa propia _END | . 8167 he is not young | START_ él no es joven _END | . 18336 she wants to dance | START_ ella quiere bailar _END | . 112211 im looking for someone who can speak portuguese | START_ busco a alguien que sepa portugués _END | . Train-Test Split . X, y = lines[&quot;english&quot;], lines[&quot;spanish&quot;] . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . X_train.shape, y_train.shape . ((99016,), (99016,)) . X_test.shape, y_test.shape . ((24754,), (24754,)) . Generator . def generate_batch(X=X_train, y=y_train, batch_size=128): &#39;&#39;&#39; Generate a batch of data &#39;&#39;&#39; while True: for j in range(0, len(X), batch_size): encoder_input_data = np.zeros((batch_size, max_length_src), dtype=&#39;float32&#39;) decoder_input_data = np.zeros((batch_size, max_length_tar), dtype=&#39;float32&#39;) decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype=&#39;float32&#39;) for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])): for t, word in enumerate(input_text.split()): encoder_input_data[i, t] = input_token_index[word] # encoder input seq for t, word in enumerate(target_text.split()): if t &lt; len(target_text.split())-1: decoder_input_data[i, t] = target_token_index[word] # decoder input seq if t&gt;0: # decoder target sequence (one hot encoded) # does not include the START_ token # Offset by one timestep decoder_target_data[i, t - 1, target_token_index[word]] = 1. yield([encoder_input_data, decoder_input_data], decoder_target_data) . Teacher Forcing . Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network. . Decoder is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called &quot;teacher forcing&quot; in this context. Effectively, the decoder learns to generate targets [t+1...] given targets [...t], conditioned on the input sequence. . Example - . Suppose, we had only 1 sentence - . English - Juan eats apples | Spanish - Juan come manzanas | . Hence, we had just 3 words in our English &amp; 5 in Spanish vocabulary. . English Vocabulary {&#39;apples&#39;: 1, &#39;eats&#39;: 2, &#39;juan&#39;: 3} Spanish Vocabulary {&#39;START_&#39;: 1, &#39;_END&#39;: 2, &#39;come&#39;: 3, &#39;juan&#39;: 4, &#39;manzanas&#39;: 5} . So our encoded input &amp; decoder input would look like - . Encoder Input Data: [[3. 2. 1.]] Decoder Input Data: [[1. 4. 3. 5. 0.]] . As the target sentence has 5 words, at timestep t during training, we set the previous timestep&#39;s t-1 actual output to 1. So essentially, we will have 5 target sentence. . Decoder Target Data: [0. 0. 0. 0. 1. 0.] # juan [0. 0. 0. 1. 0. 0.] # come [0. 0. 0. 0. 0. 1.] # manzanas [0. 0. 1. 0. 0. 0.] # _END [0. 0. 0. 0. 0. 0.] . Summary . TS1 - . Encoder Input Data - [3. 2. 1.] Decoder Input Data: [1. 4. 3. 5. 0.] Decoder Target Data: [0. 0. 0. 0. 1. 0.] # juan . TS2 - . Encoder Input Data - [3. 2. 1.] Decoder Input Data: [1. 4. 3. 5. 0.] Decoder Target Data: [0. 0. 0. 1. 0. 0.] # juan come . TS3 - . Encoder Input Data - [3. 2. 1.] Decoder Input Data: [1. 4. 3. 5. 0.] Decoder Target Data: [0. 0. 0. 0. 0. 1.] # juan come manzanas . TS4 - . Encoder Input Data - [3. 2. 1.] Decoder Input Data: [1. 4. 3. 5. 0.] Decoder Target Data: [0. 1. 0. 0. 0. 0.] # juan come manzanas _END . Model . latent_dim = 100 . # ENCODER encoder_inputs = Input(shape=(None,)) enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs) encoder_lstm = LSTM(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder_lstm(enc_emb) # discard `encoder_outputs` and only keep the states. encoder_states = [state_h, state_c] . mask_zero=True - It treats &#39;0&#39; as a padding value. As per the docs, &quot;If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1)&quot;. Which is why increased num_encoder_tokens &amp; num_decoder_tokens in cell 20 . # set up the decoder, using `encoder_states` as initial state. decoder_inputs = Input(shape=(None,)) dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True) dec_emb = dec_emb_layer(decoder_inputs) decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_outputs, dec_state_h, dec_state_c = decoder_lstm(dec_emb, initial_state=encoder_states) . WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn&#39;t meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU . decoder_dense = Dense(num_decoder_tokens, activation=&#39;softmax&#39;) decoder_outputs = decoder_dense(decoder_outputs) . Here, we add a Dense Layer that uses softmax activation on top of decoder. Notice, how for the sample sentence - Juan eats apples, the output target at each timestep looks like - . [0. 0. 0. 0. 1. 0.] # juan [0. 0. 0. 1. 0. 0.] # juan come [0. 0. 0. 0. 0. 1.] # juan come manzanas [0. 1. 0. 0. 0. 0.] # juan come manzanas _end . It is the job of the dense layer to predict this next word from the decoder_outputs . Model will take encoder inputs &amp; decoder inputs and return decoder outputs . model = Model([encoder_inputs, decoder_inputs], decoder_outputs) model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;acc&#39;]) . plot_model(model, show_shapes=True) . Training . train_samples = len(X_train) val_samples = len(X_test) batch_size = 128 epochs = 50 . csvlogger = CSVLogger(&quot;training.log&quot;) earlystopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=3) callbacks = [csvlogger, earlystopping] . history = model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs, validation_data = generate_batch(X_test, y_test, batch_size = batch_size), validation_steps = val_samples//batch_size, callbacks=callbacks ) . WARNING:tensorflow:From &lt;ipython-input-42-c7455d30101d&gt;:6: Model.fit_generator (from tensorflow.python.keras.engine.training_v1) is deprecated and will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators. Epoch 1/50 773/773 [==============================] - 434s 561ms/step - loss: 0.8506 - acc: 0.1638 - val_loss: 0.7588 - val_acc: 0.2107 Epoch 2/50 773/773 [==============================] - 436s 564ms/step - loss: 0.6943 - acc: 0.2561 - val_loss: 0.6517 - val_acc: 0.2900 Epoch 3/50 773/773 [==============================] - 435s 563ms/step - loss: 0.6062 - acc: 0.3210 - val_loss: 0.5916 - val_acc: 0.3357 Epoch 4/50 773/773 [==============================] - 434s 561ms/step - loss: 0.5472 - acc: 0.3637 - val_loss: 0.5486 - val_acc: 0.3686 Epoch 5/50 773/773 [==============================] - 434s 562ms/step - loss: 0.5002 - acc: 0.3994 - val_loss: 0.5149 - val_acc: 0.3968 Epoch 6/50 773/773 [==============================] - 433s 561ms/step - loss: 0.4612 - acc: 0.4302 - val_loss: 0.4895 - val_acc: 0.4196 Epoch 7/50 773/773 [==============================] - 435s 563ms/step - loss: 0.4284 - acc: 0.4568 - val_loss: 0.4689 - val_acc: 0.4367 Epoch 8/50 773/773 [==============================] - 435s 563ms/step - loss: 0.4000 - acc: 0.4807 - val_loss: 0.4523 - val_acc: 0.4519 Epoch 9/50 773/773 [==============================] - 436s 564ms/step - loss: 0.3749 - acc: 0.5024 - val_loss: 0.4377 - val_acc: 0.4657 Epoch 10/50 773/773 [==============================] - 436s 564ms/step - loss: 0.3524 - acc: 0.5225 - val_loss: 0.4265 - val_acc: 0.4763 Epoch 11/50 773/773 [==============================] - 437s 565ms/step - loss: 0.3322 - acc: 0.5404 - val_loss: 0.4169 - val_acc: 0.4862 Epoch 12/50 773/773 [==============================] - 437s 565ms/step - loss: 0.3137 - acc: 0.5580 - val_loss: 0.4084 - val_acc: 0.4944 Epoch 13/50 773/773 [==============================] - 435s 563ms/step - loss: 0.2968 - acc: 0.5741 - val_loss: 0.4029 - val_acc: 0.5006 Epoch 14/50 773/773 [==============================] - 435s 562ms/step - loss: 0.2812 - acc: 0.5896 - val_loss: 0.3967 - val_acc: 0.5082 Epoch 15/50 773/773 [==============================] - 434s 562ms/step - loss: 0.2669 - acc: 0.6042 - val_loss: 0.3921 - val_acc: 0.5133 Epoch 16/50 773/773 [==============================] - 434s 561ms/step - loss: 0.2537 - acc: 0.6189 - val_loss: 0.3874 - val_acc: 0.5192 Epoch 17/50 773/773 [==============================] - 434s 562ms/step - loss: 0.2413 - acc: 0.6333 - val_loss: 0.3843 - val_acc: 0.5247 Epoch 18/50 773/773 [==============================] - 435s 562ms/step - loss: 0.2301 - acc: 0.6472 - val_loss: 0.3814 - val_acc: 0.5277 Epoch 19/50 773/773 [==============================] - 438s 566ms/step - loss: 0.2199 - acc: 0.6596 - val_loss: 0.3797 - val_acc: 0.5306 Epoch 20/50 773/773 [==============================] - 436s 564ms/step - loss: 0.2105 - acc: 0.6717 - val_loss: 0.3777 - val_acc: 0.5337 Epoch 21/50 773/773 [==============================] - 434s 562ms/step - loss: 0.2015 - acc: 0.6836 - val_loss: 0.3774 - val_acc: 0.5347 Epoch 22/50 773/773 [==============================] - 434s 561ms/step - loss: 0.1934 - acc: 0.6950 - val_loss: 0.3774 - val_acc: 0.5353 Epoch 23/50 773/773 [==============================] - 434s 561ms/step - loss: 0.1859 - acc: 0.7051 - val_loss: 0.3753 - val_acc: 0.5392 Epoch 24/50 773/773 [==============================] - 433s 560ms/step - loss: 0.1788 - acc: 0.7154 - val_loss: 0.3758 - val_acc: 0.5415 Epoch 25/50 773/773 [==============================] - 433s 561ms/step - loss: 0.1724 - acc: 0.7238 - val_loss: 0.3782 - val_acc: 0.5405 Epoch 26/50 773/773 [==============================] - 433s 561ms/step - loss: 0.1664 - acc: 0.7326 - val_loss: 0.3779 - val_acc: 0.5428 . EarlyStopping callback clicked into gear at the end of epoch 26, as the validation loss only kept on increasing from 0.3753(epoch 23) to 0.3758(epoch 24), 0.3782(epoch 25) &amp; 0.3779(epoch 26) . # summarize history for accuracy plt.plot(history.history[&#39;acc&#39;]) plt.plot(history.history[&#39;val_acc&#39;]) plt.title(&#39;Model Accuracy&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;Train&#39;, &#39;Validation&#39;], loc=&#39;upper left&#39;) plt.show() . # summarize history for loss plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Model loss&#39;) plt.ylabel(&#39;loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;) plt.show() . model.save(&quot;english_to_spanish_nmt.h5&quot;) . model = load_model(&quot;english_to_spanish_nmt.h5&quot;) . WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn&#39;t meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn&#39;t meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU . files.download(&quot;english_to_spanish_nmt.h5&quot;) . files.download(&quot;training.log&quot;) . Inference . Encoder Setup . Encode the input sequence to get the encoder_states - state_h &amp; state_c . encoder_model = Model(encoder_inputs, encoder_states) . Decoder setup . Below tensors will hold the states of the previous time step. In case of the first sequence, assume - . decoder_state_input_c - state_c | decoder_state_input_h - state_h | decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] . Get the embedding of decoder output sequences. For the first sequence, it will return the embedded vector for START_ - [1., 0., 0., 0., 0.]. If the next predicted word is Juan, it will then return the embedded vector for Juan - [0., 0., 0., 1., 0.] . dec_emb2 = dec_emb_layer(decoder_inputs) . To predict the next word in the sequence, set the initial states to the states from the previous time step . decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs) . Predict the next word in the sequence using the dense layer and choose the most probable word by selecting the word with most probability from the softmax probability distribution. . decoder_outputs2 = decoder_dense(decoder_outputs2) . Final Decoder Model . Inputs - . decoder_inputs - List of word | decoder_states_inputs - previous timestep&#39;s hidden state &amp; cell state | Outputs - . decoder_outputs2 - one-hot vector represeting the predicted word | decoder_states2 - current timestep&#39;s hidden state &amp; cell state | decoder_states2 = [state_h2, state_c2] decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2) . Decode Sequence . # https://github.com/numpy/numpy/issues/15201#issue-543733072 def categorical(p): return (p.cumsum(-1) &gt;= np.random.uniform(size=p.shape[:-1])[..., None]).argmax(-1) . def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1,1)) # Populate the first character of target sequence with the start character. target_seq[0, 0] = target_token_index[&#39;START_&#39;] # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = &#39;&#39; while not stop_condition: output_tokens, h, c = decoder_model.predict([target_seq] + states_value) # Sampling a token with max probability sampled_token_index = np.argmax(output_tokens[0, -1, :]) # Sample from a categorical distribution # logits = output_tokens[0, -1, :] # sampled_token_index = categorical(np.reshape(logits, [-1, len(logits)]))[0] sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += &#39; &#39;+sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == &#39;_END&#39; or len(decoded_sentence) &gt; max_length_tar): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1,1)) target_seq[0, 0] = sampled_token_index # Update states states_value = [h, c] return decoded_sentence . Beam Search Decoding . Core idea is to keep track of the $k$ most probable partial translations. $k$ is the beam width (usually 5-10). . A hypothesis $y_{1}, y_{2}, y_{3}, ..., y_{t}$ has a score which is it&#39;s log probability: . $$score(y_{1}, y_{2}, y_{3}, ..., y_{t})= log P(y_{1}, y_{2}, y_{3}, ..., y_{t}|x)= sum_{i=1}^{t} log P(y_{i}|y_{1}, y_{2}, y_{3}, ..., y_{i-1}|x)$$ . Scores are all negative, as we are taking log of probabilities (0-1) | We search for high scoring hypothesis, keeping track of top k only at each step | . STOPPING CRITERIA . Since, different hypothesis may produce $&lt;END&gt;$ token at different timesteps. Therefore, . Once a hypothesis produces $&lt;END&gt;$ token, we regard it complete. | We then discard it and continue exploring other hypothesis. | We usually stop when, . We have say n words or sequence is say of length 50 or 100, etc. | We have certain number of completed hypothesis. | def beam_search_decoder(predictions, top_k = 3): #start with an empty sequence with zero score output_sequences = [([], 0)] #looping through all the predictions for token_probs in predictions: new_sequences = [] #append new tokens to old sequences and re-score for old_seq, old_score in output_sequences: for char_index in range(len(token_probs)): new_seq = old_seq + [char_index] #considering log-likelihood for scoring new_score = old_score + math.log(token_probs[char_index]) new_sequences.append((new_seq, new_score)) # sort all new sequences in the de-creasing order of their score output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True) #select top-k based on score # *Note- best sequence is with the highest score output_sequences = output_sequences[:top_k] return output_sequences . def decode_sequence_beam_search(input_seq): probabilities = [] # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1,1)) # Populate the first character of target sequence with the start character. target_seq[0, 0] = target_token_index[&#39;START_&#39;] # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = &#39;&#39; while not stop_condition: output_tokens, h, c = decoder_model.predict([target_seq] + states_value) # Sampling a token with max probability sampled_token_index = np.argmax(output_tokens[0, -1, :]) probabilities.append(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += &#39; &#39;+sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == &#39;_END&#39; or len(decoded_sentence) &gt; max_length_tar): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1,1)) target_seq[0, 0] = sampled_token_index # Update states states_value = [h, c] # storing multiple results outputs = [] beam_search_preds = beam_search_decoder(probabilities, top_k = 10) for prob_indexes, score in beam_search_preds: decoded_sentence = &#39;&#39; for index in prob_indexes: sampled_char = reverse_target_char_index[index] decoded_sentence += &#39; &#39;+sampled_char if (sampled_char == &#39;_END&#39; or len(decoded_sentence) &gt; max_length_tar): break outputs.append(decoded_sentence) return outputs . Utility Function . Function that makes a request at My Memory Translated to get back the English translation of predicted Spanish sentence. . url = &quot;https://api.mymemory.translated.net/get&quot; def get_translation(seq): data = {} data[&quot;q&quot;] = seq data[&quot;langpair&quot;] = &quot;es|en&quot; response = requests.post(url, data=data) translated_text = response.json()[&quot;responseData&quot;][&quot;translatedText&quot;] return translated_text . White Distance . Metric to find similarity between two sentences . def upper_case(s): return s.upper() def get_pairs(s): pairs = [] words = s.strip().split(&#39; &#39;) for word in words: for idx in range(len(word)-1): pairs.append(word[idx:idx+2]) return pairs def get_similarity(s1, s2): s1 = upper_case(s1) s2 = upper_case(s2) p1 = get_pairs(s1) p2 = get_pairs(s2) nr = 2*len(list((Counter(p1) &amp; Counter(p2)).elements())) dr = len(p1)+len(p2) return nr/dr . Training Data . train_gen = generate_batch(X_train, y_train, batch_size = 1) k=-1 . for _ in range(20): k+=1 (input_seq, actual_output), _ = next(train_gen) decoded_sentence = decode_sequence(input_seq) print(&#39;Input Sentence:&#39;, X_train[k:k+1].values[0]) print(&#39;Actual Translation:&#39;, y_train[k:k+1].values[0][6:-4]) print(&#39;Predicted Translation (Spanish):&#39;, decoded_sentence[:-4]) # predicted spanish sequence back to english print(&#39;Predicted Translation (English):&#39;, get_translation(decoded_sentence[:-4])) print(&quot;=&quot;*60, end=&quot; n n&quot;) . Input Sentence: do you know how to drive Actual Translation: ¿sabes conducir Predicted Translation (Spanish): ¿sabes cómo se levante Predicted Translation (English): Do you know how to get up ============================================================ Input Sentence: it never happened Actual Translation: nunca pasó Predicted Translation (Spanish): nunca sucedió Predicted Translation (English): It never happened. ============================================================ Input Sentence: i havent slept in days Actual Translation: no he dormido en días Predicted Translation (Spanish): no dormí en dos años Predicted Translation (English): I did not sleep in two years ============================================================ Input Sentence: i hope you enjoy your flight Actual Translation: espero que disfrute del vuelo Predicted Translation (Spanish): espero que te vayas a la leche Predicted Translation (English): I hope you go to milk ============================================================ Input Sentence: the doctor advised him to give up smoking Actual Translation: el médico le aconsejó que dejara de fumar Predicted Translation (Spanish): el médico le aconsejó que dejara de fumar Predicted Translation (English): The doctor advised him to give up smoking. ============================================================ Input Sentence: tom sees things Actual Translation: tom ve cosas Predicted Translation (Spanish): tom ve cosas Predicted Translation (English): Tom sees things. ============================================================ Input Sentence: do we have enough flour Actual Translation: ¿tenemos suficiente harina Predicted Translation (Spanish): ¿tenemos suficiente para beber Predicted Translation (English): Do we have enough to drink ============================================================ Input Sentence: tom crawled into bed just before midnight Actual Translation: tom se arrastró a la cama justo antes de medianoche Predicted Translation (Spanish): tom me quité la casa por un paseo Predicted Translation (English): tom i took off the house for a walk ============================================================ Input Sentence: im already sick Actual Translation: ya estoy enferma Predicted Translation (Spanish): ya estoy enfermo Predicted Translation (English): I am sick ============================================================ Input Sentence: what will you do when you grow up Actual Translation: ¿qué harás cuando seas mayor Predicted Translation (Spanish): ¿qué vas a poner en cuando crezcas Predicted Translation (English): What are you going to put in when you grow up ============================================================ Input Sentence: could you send me a brochure Actual Translation: ¿podrías enviarme un folleto Predicted Translation (Spanish): ¿podrías enviarme un catálogo Predicted Translation (English): Could you send me a catalog ============================================================ Input Sentence: i assure you that an error like this will never happen again Actual Translation: te aseguro que un error así no sucederá nunca más Predicted Translation (Spanish): te aseguro que no he leído ni un error por favor Predicted Translation (English): I assure you that I have not read a mistake please ============================================================ Input Sentence: tom shut himself up in his bedroom Actual Translation: tom se encerró en su cuarto Predicted Translation (Spanish): tom se encerró en su habitación Predicted Translation (English): He locked himself up in his room. ============================================================ Input Sentence: get out Actual Translation: bájate Predicted Translation (Spanish): salid Predicted Translation (English): come out ============================================================ Input Sentence: tom had a hunch that mary was seeing someone else Actual Translation: tom tuvo un presentimiento de que mary estaba viendo a alguien más Predicted Translation (Spanish): tom tuvo una foto de que mary vino a verlo Predicted Translation (English): tom had a picture of mary coming to see him ============================================================ Input Sentence: tom unlocked his briefcase Actual Translation: tom abrió los cerrojos de su maletín Predicted Translation (Spanish): tom abrió la foto Predicted Translation (English): tom opened the photo ============================================================ Input Sentence: how does the film end Actual Translation: ¿cómo termina la película Predicted Translation (Spanish): ¿cómo se llama esa película Predicted Translation (English): What&amp;#39;s that movie called ============================================================ Input Sentence: a high school student made this robot Actual Translation: un estudiante de enseñanza media hizo este robot Predicted Translation (Spanish): un estudiante de la lluvia es muy grande Predicted Translation (English): a rain student is very big ============================================================ Input Sentence: he is a good violinist Actual Translation: él es un buen violinista Predicted Translation (Spanish): él es un buen nadador Predicted Translation (English): he is a good swimmer ============================================================ Input Sentence: what time do you get up on schooldays Actual Translation: ¿a qué hora te levantas en días de clase Predicted Translation (Spanish): ¿a qué hora te levantas en australia Predicted Translation (English): What time do you get up in australia ============================================================ . train_gen = generate_batch(X_train, y_train, batch_size = 1) k=-1 for _ in range(20): k+=1 similarity_scores = [] (input_seq, actual_output), _ = next(train_gen) decoded_sentences = decode_sequence_beam_search(input_seq) acutal_sentence = y_train[k:k+1].values[0][6:-4] print(&#39;Input Sentence:&#39;, X_train[k:k+1].values[0]) print(&#39;Actual Translation:&#39;, acutal_sentence) for idx, pred in enumerate(decoded_sentences): similarity_scores.append(get_similarity(pred, acutal_sentence)) dictionary = dict(zip(similarity_scores, decoded_sentences)) dictionary = {k: v for k, v in sorted(dictionary.items(), key=lambda item: item[1], reverse=True)} closest_sentence = decoded_sentences[np.argmax(similarity_scores)] print(f&quot;Closest Predicted Sentence (Spanish): {closest_sentence[:-4]}&quot;) print(f&quot;Closest Predicted Sentence (English): {get_translation(closest_sentence[:-4])}&quot;, end=&quot; n n&quot;) decoded_sentences.remove(closest_sentence) for idx, pred in enumerate(list(dictionary.values())[:5]): print(f&#39;Predicted Translation {idx}: {pred[:-4]}&#39;) print(&quot;=&quot;*30, end=&quot; n n&quot;) . Input Sentence: do you know how to drive Actual Translation: ¿sabes conducir Closest Predicted Sentence (Spanish): ¿sabes conducir se levante Closest Predicted Sentence (English): Do you know drive up get up Predicted Translation 0: ¿sabes cómo se ponga Predicted Translation 1: ¿sabes cómo se muevan Predicted Translation 2: ¿sabes cómo se levante Predicted Translation 3: ¿sabes cómo manejar levante Predicted Translation 4: ¿sabes cómo conducir ponga ============================== Input Sentence: it never happened Actual Translation: nunca pasó Closest Predicted Sentence (Spanish): nunca pasó Closest Predicted Sentence (English): It never happened. Predicted Translation 0: nunca se Predicted Translation 1: nunca pasó Predicted Translation 2: nunca ocurrió Predicted Translation 3: nunca hizo Predicted Translation 4: nunca fue ============================== Input Sentence: i havent slept in days Actual Translation: no he dormido en días Closest Predicted Sentence (Spanish): no dormí en dos días Closest Predicted Sentence (English): I did not sleep in two days Predicted Translation 0: no me nada dos años Predicted Translation 1: no me en dos años Predicted Translation 2: no he nada dos años Predicted Translation 3: no he en dos años Predicted Translation 4: no duermo nada dos años ============================== Input Sentence: i hope you enjoy your flight Actual Translation: espero que disfrute del vuelo Closest Predicted Sentence (Spanish): espero que disfrute vayas a la leche Closest Predicted Sentence (English): I hope you enjoy going to milk Predicted Translation 0: espero que usted vayas a la leche Predicted Translation 1: espero que te vayas a la clase Predicted Translation 2: espero que te pongas a la clase Predicted Translation 3: espero que te la a la clase Predicted Translation 4: espero que disfrute vayas a la clase ============================== Input Sentence: the doctor advised him to give up smoking Actual Translation: el médico le aconsejó que dejara de fumar Closest Predicted Sentence (Spanish): el médico le aconsejó que dejara de fumar Closest Predicted Sentence (English): The doctor advised him to give up smoking. Predicted Translation 0: el médico le dijo que dejara de fumar Predicted Translation 1: el médico le aconsejó que dejara la fumar Predicted Translation 2: el médico le aconsejó que dejara de fumar Predicted Translation 3: el médico le aconsejó que dejara a fumar Predicted Translation 4: el doctor le aconsejó que dejara lo fumar ============================== Input Sentence: tom sees things Actual Translation: tom ve cosas Closest Predicted Sentence (Spanish): tom ve cosas Closest Predicted Sentence (English): Tom sees things. Predicted Translation 0: ¡tom ve cosas Predicted Translation 1: tomás ve cosas Predicted Translation 2: tom ve las Predicted Translation 3: tom ve cosas Predicted Translation 4: tom ve aquí ============================== Input Sentence: do we have enough flour Actual Translation: ¿tenemos suficiente harina Closest Predicted Sentence (Spanish): ¿tenemos suficiente harina beber Closest Predicted Sentence (English): Do we have enough flour to drink Predicted Translation 0: ¿tenemos suficiente para un Predicted Translation 1: ¿tenemos suficiente para principiantes Predicted Translation 2: ¿tenemos suficiente para otra Predicted Translation 3: ¿tenemos suficiente para escribir Predicted Translation 4: ¿tenemos suficiente para beber ============================== Input Sentence: tom crawled into bed just before midnight Actual Translation: tom se arrastró a la cama justo antes de medianoche Closest Predicted Sentence (Spanish): tom me quité la casa por un rato Closest Predicted Sentence (English): tom i took off the house for a while Predicted Translation 0: tom me quité la casa por un rato Predicted Translation 1: tom me quité la casa por un paso Predicted Translation 2: tom me quité la casa por un paseo Predicted Translation 3: tom me quité la casa por paso paso Predicted Translation 4: tom me quité la casa por paso paseo ============================== Input Sentence: im already sick Actual Translation: ya estoy enferma Closest Predicted Sentence (Spanish): ya estoy enferma Closest Predicted Sentence (English): I am sick. Predicted Translation 0: ¡estoy estoy enfermo Predicted Translation 1: yo estoy enfermo Predicted Translation 2: yo estoy enferma Predicted Translation 3: ya me enferma Predicted Translation 4: ya estoy enferma ============================== Input Sentence: what will you do when you grow up Actual Translation: ¿qué harás cuando seas mayor Closest Predicted Sentence (Spanish): ¿qué vas a quedar en cuando Closest Predicted Sentence (English): What are you going to stay on when Predicted Translation 0: ¿qué vas a quedar en cuando crezcas Predicted Translation 1: ¿qué vas a quedar en cuando Predicted Translation 2: ¿qué vas a poner en las crezcas Predicted Translation 3: ¿qué vas a poner en cuando Predicted Translation 4: ¿qué vas a estas en cuando crezcas ============================== Input Sentence: could you send me a brochure Actual Translation: ¿podrías enviarme un folleto Closest Predicted Sentence (Spanish): ¿podrías enviarme un folleto Closest Predicted Sentence (English): Could you send me a brochure? Predicted Translation 0: ¿puedes enviarme un folleto Predicted Translation 1: ¿puedes enviarme un catálogo Predicted Translation 2: ¿podrías un un catálogo Predicted Translation 3: ¿podrías traerme un catálogo Predicted Translation 4: ¿podrías enviarme un folleto ============================== Input Sentence: i assure you that an error like this will never happen again Actual Translation: te aseguro que un error así no sucederá nunca más Closest Predicted Sentence (Spanish): te aseguro que no hace leído un un error de favor Closest Predicted Sentence (English): I assure you that you do not read an error in favor Predicted Translation 0: te aseguro que no lo leído ni un error por favor Predicted Translation 1: te aseguro que no he leído un un error por favor Predicted Translation 2: te aseguro que no he leído un un error de favor Predicted Translation 3: te aseguro que no he leído ni un periódico por f Predicted Translation 4: te aseguro que no he leído ni un error de favor ============================== Input Sentence: tom shut himself up in his bedroom Actual Translation: tom se encerró en su cuarto Closest Predicted Sentence (Spanish): tom se encerró en su cuarto Closest Predicted Sentence (English): Tom locked himself in his room. Predicted Translation 0: tom se quedó en su habitación Predicted Translation 1: tom se ocultó en su habitación Predicted Translation 2: tom se escondió en su habitación Predicted Translation 3: tom se enjuagó en su habitación Predicted Translation 4: tom se encerró en su pieza ============================== Input Sentence: get out Actual Translation: bájate Closest Predicted Sentence (Spanish): bájate Closest Predicted Sentence (English): Get down. Predicted Translation 0: ¡vete Predicted Translation 1: ¡quédate Predicted Translation 2: ¡despierta Predicted Translation 3: mantente Predicted Translation 4: bájate ============================== Input Sentence: tom had a hunch that mary was seeing someone else Actual Translation: tom tuvo un presentimiento de que mary estaba viendo a alguien más Closest Predicted Sentence (Spanish): tom tuvo una foto de que mary estaba a verlo Closest Predicted Sentence (English): Tom had a picture that Mary was seeing him Predicted Translation 0: tom tuvo una reunión de que mary vino a verlo Predicted Translation 1: tom tuvo una reunión de mary mary vino a verlo Predicted Translation 2: tom tuvo una foto de que mary vino a verlo Predicted Translation 3: tom tuvo una foto de que mary vino a la Predicted Translation 4: tom tuvo una foto de que mary lo a verlo ============================== Input Sentence: tom unlocked his briefcase Actual Translation: tom abrió los cerrojos de su maletín Closest Predicted Sentence (Spanish): tom abrió la imagen Closest Predicted Sentence (English): tom opened the image Predicted Translation 0: tom abrió la primera Predicted Translation 1: tom abrió la oportunidad Predicted Translation 2: tom abrió la llave Predicted Translation 3: tom abrió la imagen Predicted Translation 4: tom abrió la fotografía ============================== Input Sentence: how does the film end Actual Translation: ¿cómo termina la película Closest Predicted Sentence (Spanish): ¿cómo se llama esa película Closest Predicted Sentence (English): What&amp;#39;s that movie called Predicted Translation 0: ¿cómo se ve esa película Predicted Translation 1: ¿cómo se va esta película Predicted Translation 2: ¿cómo se utiliza esa película Predicted Translation 3: ¿cómo se recoge esa película Predicted Translation 4: ¿cómo se llamaba esa película ============================== Input Sentence: a high school student made this robot Actual Translation: un estudiante de enseñanza media hizo este robot Closest Predicted Sentence (Spanish): un estudiante de la lluvia está muy grande Closest Predicted Sentence (English): a rain student is very big Predicted Translation 0: un estudiante de la lluvia no muy grande Predicted Translation 1: un estudiante de la lluvia está muy grande Predicted Translation 2: un estudiante de la lluvia es muy grande Predicted Translation 3: un estudiante de la lluvia es muy de Predicted Translation 4: un estudiante de la lluvia es muy bueno ============================== Input Sentence: he is a good violinist Actual Translation: él es un buen violinista Closest Predicted Sentence (Spanish): él es un buen violinista Closest Predicted Sentence (English): He is a good violinist. Predicted Translation 0: él es un buen violinista Predicted Translation 1: él es un buen vecino Predicted Translation 2: él es un buen pianista Predicted Translation 3: él es un buen nadador Predicted Translation 4: él es un buen escritor ============================== Input Sentence: what time do you get up on schooldays Actual Translation: ¿a qué hora te levantas en días de clase Closest Predicted Sentence (Spanish): ¿a qué hora te levantas en las Closest Predicted Sentence (English): What time do you get up Predicted Translation 0: ¿a qué hora te levantas por el Predicted Translation 1: ¿a qué hora te levantas por australia Predicted Translation 2: ¿a qué hora te levantas en unos Predicted Translation 3: ¿a qué hora te levantas en las Predicted Translation 4: ¿a qué hora te levantas en el ============================== . The combination of White Distance and Beam search with width 10 definitely. For exmaple - compare the result of greedy search (previous cell) vs the above combination (this cell) for some of the sentences. You would notice, that we get more and more closer to the acutal translation. . Example 1 . # greedy Input Sentence: what time do you get up on schooldays Actual Translation: ¿a qué hora te levantas en días de clase Predicted Translation (Spanish): ¿a qué hora te levantas en australia Predicted Translation (English): What time do you get up in australia . # beam search + white distance Input Sentence: what time do you get up on schooldays Actual Translation: ¿a qué hora te levantas en días de clase Closest Predicted Sentence (Spanish): ¿a qué hora te levantas en las Closest Predicted Sentence (English): What time do you get up . Example 2 . # greedy Input Sentence: he is a good violinist Actual Translation: él es un buen violinista Predicted Translation (Spanish): él es un buen nadador Predicted Translation (English): he is a good swimmer . # beam search + white distance Input Sentence: he is a good violinist Actual Translation: él es un buen violinista Closest Predicted Sentence (Spanish): él es un buen violinista Closest Predicted Sentence (English): He is a good violinist. . Example 3 . # greedy Input Sentence: i havent slept in days Actual Translation: no he dormido en días Predicted Translation (Spanish): no dormí en dos años Predicted Translation (English): I did not sleep in two years . # beam search + white distance Input Sentence: i havent slept in days Actual Translation: no he dormido en días Closest Predicted Sentence (Spanish): no dormí en dos días Closest Predicted Sentence (English): I did not sleep in two days . Here, days seems to be correct instead of years . Example 4 . # greedy Input Sentence: could you send me a brochure Actual Translation: ¿podrías enviarme un folleto Predicted Translation (Spanish): ¿podrías enviarme un catálogo Predicted Translation (English): Could you send me a catalog . # beam search + white distance Input Sentence: could you send me a brochure Actual Translation: ¿podrías enviarme un folleto Closest Predicted Sentence (Spanish): ¿podrías enviarme un folleto Closest Predicted Sentence (English): Could you send me a brochure? . Testing Data . val_gen = generate_batch(X_test, y_test, batch_size = 1) k=-1 . for _ in range(20): k+=1 (input_seq, actual_output), _ = next(val_gen) decoded_sentence = decode_sequence(input_seq) print(&#39;Input Sentence:&#39;, X_test[k:k+1].values[0]) print(&#39;Actual Translation:&#39;, y_test[k:k+1].values[0][6:-4]) print(&#39;Predicted Translation (Spanish):&#39;, decoded_sentence[:-4]) # predicted spanish sequence back to english print(&#39;Predicted Translation (English):&#39;, get_translation(decoded_sentence[:-4])) print(&quot;=&quot;*60, end=&quot; n n&quot;) . Input Sentence: we work every day but sunday Actual Translation: trabajamos todos los días excepto el domingo Predicted Translation (Spanish): trabajamos todos los días excepto los domingos Predicted Translation (English): We work every day but Sunday. ============================================================ Input Sentence: does tom want a car Actual Translation: ¿tomás quiere un auto Predicted Translation (Spanish): ¿tom quiere un coche Predicted Translation (English): Does tom want a car ============================================================ Input Sentence: i was surprised that tom spoke french so well Actual Translation: me sorprendió que tomás hablase francés tan bien Predicted Translation (Spanish): me sorprendió que tom estaba y yo nunca más Predicted Translation (English): I was surprised that Tom was and I was never again ============================================================ Input Sentence: tom asked mary for help Actual Translation: tom le pidió ayuda a mary Predicted Translation (Spanish): tom le pidió ayuda a mary Predicted Translation (English): Tom asked Mary for help. ============================================================ Input Sentence: say cheese Actual Translation: di patata Predicted Translation (Spanish): decid patata Predicted Translation (English): Say cheese. ============================================================ Input Sentence: some of these young people have legs twice as long as mine Actual Translation: algunos de estos jóvenes tienen las piernas el doble de largas que las mías Predicted Translation (Spanish): algunas personas murieron mucho de veces más tarde Predicted Translation (English): some people died a lot of times later ============================================================ Input Sentence: i dont care what she eats Actual Translation: no me interesa lo que ella coma Predicted Translation (Spanish): no me importa lo que tú Predicted Translation (English): Would it move you enough ============================================================ Input Sentence: i will have to tell him the truth tomorrow Actual Translation: tendré que decirle la verdad mañana Predicted Translation (Spanish): me encargaré de que quieras Predicted Translation (English): I&amp;#39;ll take care of what you want ============================================================ Input Sentence: weve never lived here Actual Translation: nunca hemos vivido aquí Predicted Translation (Spanish): ya no hemos vivido aquí Predicted Translation (English): we have not lived here anymore ============================================================ Input Sentence: tom and mary couldnt move the heavy trunk Actual Translation: tom y mary no podían mover el pesado tronco Predicted Translation (Spanish): tom y mary no pudieron defenderse por la salud Predicted Translation (English): tom and mary couldn&amp;#39;t defend themselves for health ============================================================ Input Sentence: what subject do you like best Actual Translation: ¿cuál es la asignatura que más te gusta Predicted Translation (Spanish): ¿qué es tu cosa que te gusta Predicted Translation (English): What is your thing that you like ============================================================ Input Sentence: he repeated his question Actual Translation: él repitió su pregunta Predicted Translation (Spanish): él pagó su pregunta Predicted Translation (English): he paid his question ============================================================ Input Sentence: my brother bought a used car so it wasnt very expensive Actual Translation: mi hermano compró un auto usado así que no era muy caro Predicted Translation (Spanish): mi hermano menor compró un vestido muy inteligente Predicted Translation (English): my younger brother bought a very smart dress ============================================================ Input Sentence: croatia is a country located in the southeastern part of europe Actual Translation: croacia es un país situado en el sudeste de europa Predicted Translation (Spanish): croacia es un país en el país por favor Predicted Translation (English): croatia is a country in the country please ============================================================ Input Sentence: he will come if you call him Actual Translation: él vendrá si tú le llamas Predicted Translation (Spanish): él vendrá si te va a la mano Predicted Translation (English): he will come if it goes to your hand ============================================================ Input Sentence: of course you can take it if you want Actual Translation: por supuesto que puedes tomarlo sí quieres Predicted Translation (Spanish): por lo puedes hacer por favor llámeme Predicted Translation (English): so can you please call me ============================================================ Input Sentence: tom shouldnt have made mary angry Actual Translation: tom no debió haber hecho enfadar a mary Predicted Translation (Spanish): tom no debería haber estado haciendo hasta mary Predicted Translation (English): tom shouldn&amp;#39;t have been doing until mary ============================================================ Input Sentence: you look stupid Actual Translation: pareces estúpido Predicted Translation (Spanish): pareces un tonto Predicted Translation (English): you look like a fool ============================================================ Input Sentence: mary was a tomboy Actual Translation: mary era un chicazo Predicted Translation (Spanish): mary es un chicazo Predicted Translation (English): Mary is a guy ============================================================ Input Sentence: i think youre the only one who cares Actual Translation: yo creo que usted es el único al que le importa Predicted Translation (Spanish): creo que eres la única a la decisión Predicted Translation (English): I think you are the only one to decide ============================================================ .",
            "url": "/notes/seq2seq/nlp/machine-translation/2020/06/23/Seq2Seq.html",
            "relUrl": "/seq2seq/nlp/machine-translation/2020/06/23/Seq2Seq.html",
            "date": " • Jun 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "How to win a data science competition (Week 4)",
            "content": "How to win a data science competition (Week 4) . Hyperparameter Tuning . Select the most important parameters | Understand how exactly they influence the training | Libraries - . HyperOpt | Scikit-optimize | GPyOpt | RoBo | . HyperOpt . [HyperOpt Tutorial | Kaggle](https://www.kaggle.com/fanvacoolt/tutorial-on-hyperopt) | . | Medium Blog | Working Example | All Classifiers Example | . Bayesian optimisation takes into account past evaluations when choosing the hyperparameter set to evaluate next. By choosing its parameter combinations in an informed way, it enables itself to focus on those areas of the parameter space that it believes will bring the most promising validation scores. . SEARCH SPACE . Bayesian Optimisation operates along probability distributions for each parameter that it will sample from. We specify a probability distribution for the hyperparameters to sample from. One could choose from the following - . hp.choice | hp.uniform | hp.normal | hp.lognormal | . OBJECTIVE FUNCTION . The objective function takes in hyperparameters and outputs a single real-valued score that we want to minimize (or maximize). It simply takes in a set of hyperparameters and outputs a score that indicates how well a set of hyperparameters performs on the validation set. For example - RMSE . The entire concept of Bayesian model-based optimization is to reduce the number of times the objective function needs to be run by choosing only the most promising set of hyperparameters to evaluate based on previous calls to the evaluation function. . Note Objective Function is usually the loss function. By default hyperopt tries to minimize the objective function. . SURROGATE FUNCTION . Surrogate Function is an approximation of objective function. It is used to propose parameter sets to the objective function that likely yield an improvement. HyperOpt uses something called Tree Parzen Estimator (TPE). . SELECTION FUNCTION . The selection function is the criteria by which the next set of hyperparameters are chosen from the surrogate function. The most common choice of criteria is Expected Improvement. . XGBoost . INCREASING MODEL’S CAPACITY . max_depth - Increasing this value will make the model more complex and more likely to overfit. Usually start in between [3, 7]. Increase only if there’s an improvement in score. | subsample - Controls the fraction of objects to use when fitting a true. Ranges between [0-1]. Has some sort of regularisation effect and prone to overfitting. | colsample_bytree - Controls the fraction of features to consider at every split. [0-1] | eta - Learning rate | num_round - Number of trees to build. | . DECREASING MODEL’s CAPACITY . min_child_weight - The larger min_child_weight is, the more conservative the algorithm will be. Ranges $[0, _{inf}]$ | gamma - Minimum loss required to make a further split. Larger values results in conservative model. Ranges $[0, _{inf}]$ | lambda - L2 regularization. | alpha - L1 regularization | . Usually after finding the appropriated values for num_round and eta. Divide eta by alpha &amp; multiply num_round by alpha, this will usually give a slightly better result. . RandomForest/ExtraTrees . Unlike in Gradient Boosting Trees &amp; it’s variants, RF builds trees parallely &amp; hence increasing the number of trees won’t overfit the model. . n_estimators - Start from small values say 10 &amp; check the time required to fit these 10 trees. If its not too high then use a larger value. | max_depth - Usually tree depth for RF is greater then GBDTs. | max_features | min_samples_leaf | . Neural Networks . INCREASING MODEL’S CAPACITY . Number of neurons per layer | Number of layers | Adaptive Methods (Adam, Adagrad) - Leads to overfitting sometimes | Batch Size - Larger batch size causes overfitting | . DECREASING MODEL’S CAPACITY . SGD+momentum - Even though it leads to slower learning. The model generalises better. | Regularization - L2/L1, Dropout/Dropconnect, | . Some people follow the rule - If you increase your batch size by a factor of alpha, increase your learning rate by the same factor as well. . Cross Validation Strategy . Creating a validation strategy, is to create a validation approach that resembles what you are being tested on. The validation data should be consistent with the test data. . TIME BASED . Always have past data predicting the future data. Also the intervals need to be similar with test data. So, if test data is 3 months in the future, the validation data should also be 3 months future compared to the training data. . STRATIFIED . Different entities than train? Suppose the test data has entities which are not present in the training data, then the validation dataset should be created such that there are entities in it which are not present in the train data. . Ensembling . Smaller data requires simpler ensembling techniques like averaging. | Bigger data works well with Stacking like techniques | Statistic and Distance based feature engineering . Calculating various statistics of one feature grouped by another | Features derived from neighbourhood analysis of a given point | . Matrix Factorizations . Coursera Matrix Factorization . General Notes . MF is a very general approach for dimensionality reduction and feature selection | It can be applied for transforming categorical features into real-valued features | . Implementation Notes . Matrix factorisation can be applied to only a subset of columns if desired | We can use matrix factorisation to get another representation of the same data, especially useful for ensemble models | Loss of information | The number of latent factors to use must be treated as a hyperparamater &amp; must be tuned. | . NMF (Non-negative Matrix Factorisation) . It transforms data in a way that makes it more suitable for decision trees. NMF cannot be applied on negative values. “Standardized” means that every feature column has zero mean and unit variance. This implies that we have negative values and cannot apply NMF. . X_all = np.conctenate([X_train, X_test]) pca.fit(X_all) X_train_pca = pca.transform(X_train) X_test_pca = pca.transform(X_test) . Manifold learning in general are non-linear methods of dimensionality reduction. Apply t-SNE to concatenation of train and test and split projection back. .",
            "url": "/notes/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html",
            "relUrl": "/hyperparameter%20tuning/datascience/kaggle/2019/06/29/week4.html",
            "date": " • Jun 29, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "How to win a data science competition (Week 3)",
            "content": "How to win a data science competition (Week 3) . Regression Metrics . . Mean Squared Error (MSE) . MSE is the most commonly used metric | Follows a parabolic curve | Optimal constant -&gt; Mean value of the target column | Error is always greater than 0 and 0 for a perfect model | Sensitive to outliers, as the values tend to be more biased towards the outliers | . Root Mean Squared Error (RMSE) . RMSE = sqrt(MSE) | Square root is taken to make the scale of the error same as the scale of the target | Every minimiser of MSE is also a minimiser of RMSE i.e. MSE(a) &gt; MSE(b) implies RMSE(a) &gt; RMSE(b) | We can minimise for MSE instead of RMSE; however this is not true for gradient based methods | . R-Squared . It’s difficult to judge whether our model is good or bad just by looking at the values of MSE &amp; RMSE | So, we should compare it against the baseline model which is mean for MSE &amp; RMSE When MSE is 0 ie the numerator is 0, then R-squared is 1 | When our model performs just like the baseline model or worse, then R-squared is 0 | . | To minimise R-squared we can still minimise MSE | . Mean Absolute Error (MAE) . MAE is less sensitive than MSE for outliers | Optimal constant -&gt; Median of the target column | If there are outliers use MAE, but if there are unexpected values that is useful use MSE | . Mean Absolute Percentage Error (MAPE) . . Mean Absolute Percentage Error (MAPE) is a weighted version of MAE | Optimal constant (MAPE) =&gt; Weighted median of the target values | MAPE is useful if you are interested in minimising the relative error rather than absolute error. For example, if the error between 9 &amp; 10 is much worse than error between 999 &amp; 1000, we can use this loss function | MAPE is undefined when the actual value is 0 | MAPE values can grow very large if the actual values (denominator) themselves are very low | MAPE is biased towards predictions that are lower than the actual values | MAPEs greater than 100% can occur. | . Example . yhat=20, yact=10, n=1 then MAPE = 50% yhat=10, yact=20, n=1 then MAPE = 100% . Mean Squared Percentage Error (MSPE) . Mean Squared Percentage Error (MSPE) is a weighted version of MSE | As the target value for both of them increases, the curves flatten out | Optimal constant (MSPE) =&gt; Weighted mean of the target values | . Root Mean Square Logarithmic Error (RMSLE) . [RMSLE](https://hrngok.github.io/posts/metrics/#Root-Mean-Squared-Logaritmic-Error-(RMSLE) . RMSLE = RMSE(log(y_true+1), log(y_pred+1)) . from sklearn.metrics import mean_squared_log_error np.sqrt(mean_squared_log_error(y_true, y_pred)) . RMSE is the Root Mean Squared Error of the log-transformed predicted and log-transformed actual values. | Works for non-negative values only. A constant is added to the predictions and actual values in case the values are 0 as logarithm(0) is not defined | . When to Use? . Cares about relative error more than absolute error | We don’t want to penalize big differences when both the predicted and the actual are big numbers. | . Acutal=30, Predicted=40, RMSE=10, RMSLE=0.27 Actual=300, Predicted=400, RMSE=100, RMSLE=0.28 . We want to penalize under estimates more than over estimates | . Actual = 600, Predicted = 1000, RMSLE = 0.51 Actual = 1000, Predicted = 1400, RMSLE = 0.36 Sales &amp; Inventory products, where having extra supply might be more preferable to not being able to providing product as much as the demand . Poisson Loss . The Poisson loss is a loss function used for regression when modeling count data. Use the Poisson loss when you believe that the target value comes from a Poisson distribution and want to model the rate parameter conditioned on some input. Example - number of customers entering a shop, number of emails in a day, etc . Regression Loss . Huber Loss . Huber Loss is a combination of MAE &amp; MSE. It is quadratic for smaller errors and linear otherwise i.e it acts like MSE for error closer to 0 and like MAE elsewhere. It is differentiable at 0 unlike MAE. How small that error has to be to make it quadratic depends on a hyperparameter, 𝛿 (delta). It approaches MAE when 𝛿 ~ 0 and MSE when 𝛿 ~ ∞. . . . def huber(true, pred, delta): loss = np.where(np.abs(true-pred) &lt; delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2)) return np.sum(loss) . The downside being that we might need to train the hyper parameter 𝛿 (delta) which is an iterative process. . Log Cosh Loss . Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. . . It acts like a quadratic loss for smaller values of x and a shifted version of abs(x) for larger values of x. It is also twice differentiable, which is very useful for methods like XGBoost which use Hessian matrix for optimisation. . def logcosh(true, pred): loss = np.log(np.cosh(pred - true)) return np.sum(loss) . Classification Metrics . Soft Labels - Classifier’s probability scores | Hard Labels - Label associated with a prediction; usually argmax(soft_labels) | . Matthews Correlation Coefficient . Matthews Correlation Coefficient . . Used for binary classification | It is regarded as a balanced measure which can be used even if the classes are of very different sizes. | MCC is in essence a correlation coefficient between the observed and predicted binary classifications | A coefficient of +1 represents a perfect prediction (FP=FN=0), 0 no better than random prediction and −1(TP=TN=0) indicates total disagreement between prediction and observation. | MCC is also perfectly symmetric, so no class is more important than the other; if you switch the positive and negative, you’ll still get the same value. | . F1-Score . F1-Score . Precision . . Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive. | Precision is a good measure to use if the cost of false positive is high Eg. In a spam detector, if a non-spam mail is classified as spam, then the user might lose an important mail. | . Recall . . Recall actually calculates how many of the actual positives our model capture through labeling it as Positive | Recall is a good measure to use if the cost of false negative is high | . Accuracy . It is the fraction of correctly classified objects | The best constant for accuracy =&gt; Always predicting the class with highest frequency | Accuracy also doesn’t care how how confident the classifier is in the predictions i.e. it doesn’t care about the predicted class probabilities. It is harder to optimise and it only cares about the class labels. | It is most used when all the classes are equally important. | . Accuracy = (TP + TN) / (TP + TN + FP + FN) . LogLoss . LogLoss cares about soft labels i.e. class probabilities. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. | The probabilities always sum upto 1 . M - number of possible class labels (dog, cat, fish) | log - the natural logarithm | y - a binary indicator (0 or 1) of whether class label c is the correct classification for observation o | p - the model’s predicted probability that observation o is of class c | . | LogLoss hugely penalises wrong answers as can be seen in the above graph (actual class=1). As we move towards the correct prediction the loss decreases &amp; there’s an upward curve when we predict the class as 0 | Optimal Constant (LogLoss) =&gt; Set probabilities as frequency of the classes. [0.1, 0.9] if number of class1 samples is 10 and number of class2 samples is 90. | . AUC ROC . Tries all possible values as threshold &amp; then aggregates their scores. | Used only for binary tasks. Depends on the ordering of predictions, not on absolute values. | ROC is a probability curve and AUC represents degree or measure of separability. | Random prediction leads to AUC = 0.5 | . AUC ROC Curve . An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. | . Pairs Ordering . It is the ratio of correctly ordered pair upon total number of pairs. Ordering/Sorting of samples based on score is important. | AUC is the probability that score for the false positive will be greater than the score for true positive. | . AUC = # correctly ordered pairs / # total number of pairs . Example - Suppose the ordering based on probability score is [TP, FP, TP, TP, FP, FP] then there are 7 instances/pairs when score(FP) &gt; score(TP) out of 9 possible pairs. So, AUC = 7/9 TP is when a model has correctly classified a sample as positive class | FP is when a model has wrongly classified a sample as positive class | TN is when a model has correctly classified a sample as negative class | FN is when a model has wrongly classified a sample as negative class | . | . Quiz . How would multiplying all of the predictions from a given model by 2.0 (for example, if the model predicts 0.4, we multiply by 2.0 to get a prediction of 0.8) change the model’s performance as measured by AUC? . No change. AUC only cares about relative prediction scores. AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. This is clearly not the case for other metrics such as squared error, log loss, or prediction bias (discussed later). . LogLoss value for N number of classes with constant prediction. . log(N) . Cohen’s Kappa . The Kappa statistic (or value) is a metric that compares an Observed Accuracy with an Expected Accuracy (random chance). In addition, it takes into account random chance (agreement with a random classifier), which generally means it is less misleading than simply using accuracy as a metric. . The kappa statistic is often used as a measure of reliability between two human raters. In supervised learning, one of the raters reflects “ground truth”, and the other “rater” is the machine learning classifier. . Cohen’s Kappa = 1 - [(1 - accuracy) / (1 - baseline)] . Alternativey, since error=1 - accuracy, . Cohen’s Kappa = 1 - (error / baseline_error) . It can also, be written as - . Kappa = (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy) . Cohen’s Kappa ranges from [-1, 1] . Example . Assume Confusion Matrix for a binary classification task as follows .   Cats Dogs . Cats | 10 | 7 | . Dogs | 5 | 8 | . Observed Accuracy . It is simply the number of instances that were classified correctly throughout the entire confusion matrix ie the number of instances when the ground truth and classifier both agreed to a particular class for a sample. . Total Number of Instances = 30 Observed Accuracy = (10+8 ) / 30 = 0.6 . Expected Accuracy . Marginal Frequency (Cats) = [(10+7) * (10+5)] / 30 = 8.5 Marginal Frequency (Dogs) = [(7+8) * (5+8)] / 30 = 6.5 Expected Accuracy = [Marginal Frequency (Cats) + Marginal Frequency (Dogs)] / 30 = 0.5 . Kappa . Kappa Score = (observed accuracy - expected accuracy)/(1 - expected accuracy) Kappa Score = (0.60 - 0.50) / (1 - 0.50) = 0.20 . Quadratic Weighted Kappa . Quadratic Weighted Kappa . from sklearn.metrics import cohen_kappa_score, confusion_matrix qwk = cohen_kappa_score(actuals, preds, weights=&quot;quadratic&quot;) . Target &amp; Metric . Target Metric is what we want to optimise. It is how the model is eventually evaluated. | But no one really knows how to optimise target metrics efficiently. So, instead, we use Loss Functions, which is easy to optimise. Example it is not easy to optimise Accuracy Score, so we optimise for LogLoss &amp; eventually evaluate our model using Accuracy Score. | However, sometimes, the models can however optimise target metrics directly example MSE, LogLoss. | Sometimes it is not possible to optimise target metric directly but we can somehow preprocess the train data and use a model with a metric or loss function which is easy to optimise. For example, optimising MSPE or MAPE is not easy, but we can instead optimise MSE | Sometimes, we will optimise incorrect metrics but we will post-process to fit evaluation metric better | A technique that always works is early stopping. Suppose that we have to optimise for M2, but cannot optimise directly. So, we instead optimise for metric M1 and monitor metric M2 on validation set. We stop when the model starts overfitting on M2. | . Regression Metrics Optimization . MSE . Most of the libraries have MSE implemented as a loss function, so we can directly optimise for MSE. Synonyms - L2 Loss . Tree-Based :- LightGBM, RandomForest and XGBoost | Linear-Model :- SGDRegressor, Vowpal Vabbit | Neural Nets :- Pytorch, Keras, TF | . MAE . MAE is another commonly used metric, so most of the libraries can optimise for MAE directly. Synonyms - L1 Loss, Quantile loss, Huber loss . Tree-Based :- LightGBM, RandomForest, XGBoost | Neural Nets :- Pytorch, Keras, TF | . RMSLE . Train . Transform the target | . z = log(y+1) . Fit a model with MSE loss | . Test . Transform the prediction probabilities back | . y = exp(z) - 1 . Classification Metrics Optimization . LogLoss . Similar to MSE in terms of popularity &amp; hence implemented in almost all the major libraries. . Tree-Based :- LightGBM and XGBoost | Linear-Model :- SGDRegressor, Vowpal Vabbit | Neural Nets :- Pytorch, Keras, TF | . AUC . Optimise pairwise loss for optimising AUC . Tree-Based :- LightGBM and XGBoost | Neural Nets :- Pytorch, Keras, TF | . F1-Score . def f1_metric(y_true, y_pred): y_pred = K.round(y_pred) tp = K.sum(K.cast(y_true*y_pred, &#39;float&#39;), axis=0) tn = K.sum(K.cast((1-y_true)*(1-y_pred), &#39;float&#39;), axis=0) fp = K.sum(K.cast((1-y_true)*y_pred, &#39;float&#39;), axis=0) fn = K.sum(K.cast(y_true*(1-y_pred), &#39;float&#39;), axis=0) p = tp / (tp + fp + K.epsilon()) r = tp / (tp + fn + K.epsilon()) f1 = 2*p*r / (p+r+K.epsilon()) f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1) return K.mean(f1) def f1_loss(y_true, y_pred): tp = K.sum(K.cast(y_true*y_pred, &#39;float&#39;), axis=0) tn = K.sum(K.cast((1-y_true)*(1-y_pred), &#39;float&#39;), axis=0) fp = K.sum(K.cast((1-y_true)*y_pred, &#39;float&#39;), axis=0) fn = K.sum(K.cast(y_true*(1-y_pred), &#39;float&#39;), axis=0) p = tp / (tp + fp + K.epsilon()) r = tp / (tp + fn + K.epsilon()) f1 = 2*p*r / (p+r+K.epsilon()) f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1) return 1 - K.mean(f1) . Mean Encoding . Link 1 Link 2 . We encode each level of categorical variable with corresponding target mean. | The more complicated and non linear is the feature target dependency more effective is mean encoding | Greater the number of level in categorical features is a good indicator of using mean encodings | Prone to overfitting if the levels in categorical features in train and test datasets are different | Unlike other encoding techniques, mean encoding imposes an ordering | . Ways to construct mean encoding . Likelihood = #ones / (#ones + #zeros) Weight of Evidence = log(#ones / #zeros) * 100 Count = #ones Difference = #ones - #zeros . Possible Leaks . Calculate means only on the train data. Then calculate mean encodings using by applying map function on both the train &amp; test data. . # col = categorical feature # target = target column means = X_train.groupby(col).target.mean() train[col + &#39;_mean_target&#39;] = train[col].map(means) val[col + &#39;_mean_target&#39;] = val[col].map(means) . Regularization . Expanding Mean . Introduces least amount of leakage | No hyper parameters to tune | Built in CatBoost | . cumsum = train.groupby(col)[&#39;target&#39;].cumsum() - train[&#39;target&#39;] cumcnt = train.groupby(col).cumcnt() train_new[col+&#39;_mean_target&#39;] = cumsum/cumcnt . Regression . Encode your categorical variable with the mean of the target. For every category, you calculate the corresponding mean of the target (among this category) and replace the value of a category with this mean. More flexible compared to classification tasks as you can use a variety of statistics like median, standard deviations, percentiles, etc. .",
            "url": "/notes/evaluation%20metrics/losses/datascience/kaggle/2019/06/26/week3.html",
            "relUrl": "/evaluation%20metrics/losses/datascience/kaggle/2019/06/26/week3.html",
            "date": " • Jun 26, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "How to win a data science competition (Week 2)",
            "content": "How to win a data science competition (Week 2) . 1. EDA . Better understand the data | Build an intuition about the data | Generate Hypothesis | Gain insights | . 1.1 Building intuition about data . Get domain knowledge | Understand the data &amp; how people usually tackle the problem | Understand different features, how they relate with each other &amp; how does it affect the target feature | It helps to deeper understand the problem | . Check if the data is intuitive/satisfactory | And agrees with domain knowledge | Looking at quantiles is useful | . Understand how the data was generated | Crucial to setup proper validation | . 1.2 Anonymised Data . 1.2.1 Explore Individual Features . Guess the meaning of the features | Guess the types of the column | . Play around with . pd.Series.value_counts pd.DataFrame.dtypes mean and standard deviation pd.DataFrame.sort_values(by=&quot;&quot;) np.diff . You never know what you might discover . 1.3 Visualization Tools . 1.3.1 Visualization of Individual Features . Misleading Histograms . plt.hist(x) # if degenerate graph; like all values pointing to 0 plt.hist(log(x)) # horizontal line implies repeating values &amp; the missing vertical lines implies well shuffled data plt.plot(x, &#39;.&#39;) df.describe() x.value_counts() x.isnull() . 1.3.2 Visualization of feature relations . Scatter Plots | Can be used to check if the data distribution between train and test set are the same | . Scatter Matrix | Correlation Matrix | 2. Data Cleaning . Concatenating train and test data first . 2.1 Remove constant columns . Remove features that have only 1 value for the entire feature. . traintest.nunique(axis==1) == 1 . Sometimes it might happen that there is constant value in the train data but multiple values for that feature in the test data. . Remove that feature | Train multiple models, each model for a different value in that feature | . 2.2 Remove duplicated columns . 2.2.1 Numerical . traintest.T.drop_duplicates() . 2.2.2 Categorical . # label encode each categorical feature as numeric &amp; repeat as above for f in categorical_feats: traintest[f] = traintest[f].factorize() traintest.T.drop_duplicates() . I usually look at the value_counts of each feature. If the feature has low cardinality, then category-wise count for 2 same features will also be the same. . 2.3 Others . Find duplicated rows and understand why they are duplicated | Check if the dataset is shuffled | . 3. Validation types . Holdout - Divide train into train and validation such that each sample belongs either to train or validation. There should be no repetition. In case, there are duplicated samples in the data, it might happen that one of the duplicated example is in val, this would lead to higher scores and improper hyperparameter selection. | . sklearn.model_selection.ShuffleSplit . K-Fold - K times slower than Holdout. Different than repeating Holdout K times in the sense, that it’s entirely possible that a sample may never be a part of validation set or might be a part of validation set multiple times. K-Fold however ensures that each sample is a part of validation set only once. | . sklearn.model_selection.Kfold . Leave-one-out Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset). | In the end you will get LOO predictions for every sample in the trainset and can calculate loss. | . | . Small amount of data Faster model to train . Stratification - Preserves same target distribution over different folds Small training data | Unbalanced datasets | Multiclass classification problem | . | . 4. Data splitting strategies . We should make train/validation split to mimic the train/test split. . Random/rowwise split | Time based split - Similar are moving window validation split | Id-based split | Combined split | . Quizes . Suppose we are given a huge dataset. We did a KFold validation once and noticed that scores on each fold are roughly the same. Which validation type is most practical to use? | Use holdout validation scheme as the data is homogenous | . If validation scores differ noticeably for each fold in KFold, we should stick with KFold in order to select statistically significant changes in scores while tuning a model. . | The features we generate depend on the train-test data splitting method. Is this true? . | True | . Performance increase on a fixed cross-validation split guaranties performance increase on any cross-validation split. Incorrect. You can overfit to the specific CV-split. You should change your split from time to time to reduce the chance of overfitting. . | On Kaggle, make submissions which are . | Performing best on validation data - Assuming that train and test data distribution is same. | Performing best on public LB - Assuming that train and test data distribution is different. | . Data Leaks . Basic Data Leaks . Row Based - Data may sometimes be shuffled by target variable. So simply adding row number might improve the score. | Id Based - Sometimes, ID may contain traces of information connected to target variable. | Meta-Data - Example image creation date, camera resolution etc in a classical cats vs dogs contest, where cats images were taken before dogs or taken using specific camera. | .",
            "url": "/notes/datascience/kaggle/eda/cross-validation/2019/06/19/week2.html",
            "relUrl": "/datascience/kaggle/eda/cross-validation/2019/06/19/week2.html",
            "date": " • Jun 19, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "How to win a data science competition (Week 1)",
            "content": "How to win a data science competition (Week 1) . Real World ML Pipeline . Understanding of business problem | Problem Formalization | Data Collection | Data Preprocessing | Modelling | Way to evaluate model in real life | Way to deploy model | . Data Science Competitions . Problem formalization (might be necessary sometimes) | Data Collection (possible in few competitions) | Data Preprocessing | Modelling | . Differences . Usually the hardest part of problem formalisation and evaluation metrics is already done. | Deployment is out of scope. | Model complexity, speed &amp; memory consumption doesn’t matter | . Resources . Random Forest | ExtraTrees Classifier | . 1. Review . 1.1 Linear Model . Especially good for sparse high dimensional data. Support Vector Machine (SVM) is a linear model with special loss function. Even with “kernel trick”, it’s still linear in new, extended space. Libraries: Scikit-learn and Vowpal Wabbit . 1.2 Tree Based Model . Decision trees used divide-and-conquer technique to recursively divide spaces into sub-spaces. Tree-based models can work very well with tabular data. . However, tree-based approaches cannot easily capture linear dependencies since it requires many splits. . . ExtraTrees classifier always tests random splits over fraction of features (in contrast to RandomForest, which tests all possible splits over fraction of features) . Libraries: Scikit-learn, XGBoost, LightGBM . 1.3 No Free Lunch Theorem . “There is no method which outperforms all others for all tasks“ . “For every method we can construct a task for which this particular method will not be the best“ . . SVMs have a linear boundaries | Decision trees have horizontal and vertical splits | Random Forest has many more of these horizontal and vertical axes and are relatively smoother | Naive Bayes has a smoother boundary compared to Neural Networks | . 2. Preprocessing . 2.1 Numeric Features . Preprocessing depends on the type of model we use. . Tree based models (Decision Trees) | Non-tree based models (KNNs, Neural Nets, Linear Models) | 2.1.1 Scaling . Non-tree based models are affected by the scale of the features. Different feature scaling results in different models quality. On the other hand, Decision Trees try to find the best split for a feature, no matter the scale. . Explanation . Nearest Neighbours The scale of features impacts the distance between samples. | With different scaling of the features nearest neighbours for a selected object can be very different. | . | Linear Models / Neural Networks Amount of regularization applied to a feature depends on the feature’s scale. | Optimization methods can perform differently depending on relative scale of features. | . | Others KMeans, SVMs, LDA, PCA, etc | . | . MinMax Scaler - The distribution of the feature before and after scaling remains the same. The value range is 0 to 1. | X = (X — X.min) / (X.max — X.min) . sklearn.preprocessing.MinMaxScaler . Standard Scaler - We always get a standard normal distribution. | X = (X — X.mean) / X.std . sklearn.preprocessing.StandardScaler . 2.1.2 Winsorization . Treating outliers by clipping values between two ranges - upper bound and lower bound. Say 1 &amp; 99 percentile of the feature values. . 2.1.3 Rank Transformation . Sorts an array and changes their values to indices. Similar to binning. . Example: rank([-100, 1, 1e-5]) = [0, 1, 2] rank([1000, 1, 10]) = [2, 0, 1] . Linear Models, KNN and Neural Networks usually benefit from this transformation. . Use scipy.stats.rankdata to create ranks. For test data, either store the mapping of value ranges to indices or concatenate train &amp; test data before applying the rank transformation. . 2.1.4 Log Transformation . Helps all non-tree based models especially neural nets. . . # log transformation np.log(1 + x) . Skewness transformation | This transformation is non-linear and will move outliers relatively closer to other samples. Also, values near zero become more distinguishable. | 2.2 Categorical Features . Excellent article . 2.2.1 Label Encoding . Works well with tree based methods. Linear methods struggle to extract meaning out of label encoded features. Categories encoded with numbers that close are to each other (usually) are not more related then categories encoded with numbers that far away from each other. . Categorical features are ordinal in nature | When the number of categorical features in the dataset is huge | Methods - . Alphabetically Sorted [S, C, Q] -&gt; [2, 1, 3] | sklearn.preprocessing.LabelEncoder . Order of Appearance [S, C, Q] -&gt; [1, 2, 3] | Pandas.factorize . 2.2.2 Frequency Encoding . Encode categories on the frequency of their occurrence. Preserves information about value distribution. Example: For a given list [S, C, Q, S, S, C, Q, S, C, S] the feature column can be encoded by replacing [S, C, Q] -&gt; [0.5, 0.3, 0.2] . Can be useful for both tree based &amp; non-tree based models. . 2.2.3 One hot Encoding . Works best for non-tree based models. One-hot encoded features are already scaled as the maximum value is 1 and minimum value is 0. Hence, suitable for non-tree based models which suffer from scaling issues. . On each split, trees can only separate one category from the others. So greater the number of categories, greater will be the number of splits. . 2.2.4 Mean Target Encoding . Encoding categorical variables with a mean target value (and also other target statistics) is a popular method for working with high cardinality features, especially useful for tree-based models. Mean encodings let models converge faster. Useful when working with high cardinality categorical features. Easy to overfit. . means = df.groupby(&#39;X&#39;)[&#39;y&#39;].mean() df[&#39;X&#39;] = df[&#39;X&#39;].map(means) . It is useful to apply target encoding when the number of samples of each category type belonging to the target value equal to say 1 is reasonable i.e if y has values only 0 and 1 &amp; say for the category a belonging to x_0 doesn’t have any target value as 1, then it’s mean will be 0. . For continous target value, feature is replaced by average target value. Example - if profession=teacher is the categorical feature &amp; salary is target, then teacher is replaced by average salary of teachers in the training set . 2.2.5 Binary Encoding . Binary encoding for categorical variables, similar to onehot, but stores categories as binary bitstrings. . First the categories are encoded as ordinal, then those integers are converted into binary code, then the digits from that binary string are split into separate columns. This encodes the data in fewer dimensions that one-hot, but with some distortion of the distances. . Different categories may share some of the same features. . 3 Datetime . 3.1 Periodicity . Useful to capture repetitive patterns in data. We can add features like . Day number in week | Month | Season | Year | Second | Minute | Hour | . 3.2 Time passed since a particular event . Case 1 - . In this case, all the samples become comparable between each other on one time scale. Days passed since January 1, 2000. . Case 2 - . In this case, the date will depend on the sample we are calculating this for. For example - number of days since last holiday, number of days since last weekend, number of days to the next holiday, etc. . 3.3 Difference between Dates . Number of days between two events. Example - Subtracting end_date - start_date gives number of years loan amount was paid. . 4. Coordinates . 4.1 Distance based Features . Find out the most interesting points in the map say the best school/hospital in the town, a museum, etc and calculate distance of the samples to this point. Add them as a feature in the training and test dataset. . 4.2 Aggregated Statistics . Calculate aggregated statistics for objects surrounding areas such as the total number of flats in the vicinity which can then be interpreted as areas of popularity.’ . Or you could calculate average price flat grouped by say pin code, area which would indicate expensiveness. . 5. Missing Values . Missing values can be represented in any way not necessarily as NaN. Some examples are -1, 999, , empty string, NaN, etc. | Sometimes missing values can contain information by themselves. | . 5.1 Imputation . Use a value outside the range of the normal values for a variable. like -1 ,or -9999 etc. | Replace with a likelihood – e.g. something that relates to the target variable. | Replace with something which makes sense. For example - sometimes null may mean zero | You may consider removing rows with many null values | Try to predict missing values based on subsets of know values. For example - in time series data, where rows are not independent of each other, then a missing value can be replaced by say averaging values from within a window | . 5.2 Adding isNull column . For each of the numerical features we can add an isNull column representing whether a particular row for a column in discussion contains empty values (NaN, NaT, None). Useful specially for tree-based methods &amp; neural nets. The downside is that we double the number of columns. . 5.3 General . Some models like XGBoost and CatBoost can deal with missing values out-of-box. These models have special methods to treat them and a model’s quality can benefit from it | Do not fill NaNs before feature generation | Impute with a feature mean/median | Remove rows with missing values | Reconstruct the missing values | . 5.4 Issue while generating new feature from existing feature which has null values . Order should be - . Missing Value Imputation -&gt; Feature Generation . We should be very careful about replacing missing values before the feature generation. . 6. Feature Extraction in Text and Images . 6.1 Bag of Words . N-grams can help utilise local context around each word because n-grams encode sequences of words. | It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. | N-grams features are typically sparse. N-grams deal with counts of words occurrences, and not every word can be found in a document. | Bag of words usually produces longer vectors than Word2Vec. Number of features in Bag of words approach is usually equal to number of unique words, while number of features in w2v is restricted to a constant, like 300 or so. | Meaning of a value in BOW matrix is the number of a word’s occurrences in a document. Values in vectors cannot be interpreted. | Word2Vec and Bag of words give different results. We can combine them to get more variety of results. | 6.1.1 Countvectorizer . Count the occurence of each word in a given document. . Needs scaling at the end to be use in linear models, as most occurring words will have higher counts and assume more importance. This is overcome by tf-idf vectorizer. . sklearn.feature_extraction.text.CountVectorize . 6.1.2 Tf-idf vectorizer . TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. . MAKING DOCUMENTS/SENTENCES OF DIFFERENT LENGTHS MORE COMPARABLE . Term Frequency measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization. Term-frequency (TF) normalises sum of the column values to 1. . TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). . BOOST MORE IMPORTANT FEATURES . Normalizing feature by the inverse fraction of documents. In this case features corresponding to frequent words will be scaled down compared to features corresponding to rarer words. . IDF(t) = log(Total number of documents / Number of documents with term t in it). . IDF scales features inversely proportionally to a number of word occurrences over documents . 7. Geographic Data . 7.1 ZIP Codes . Letting zip code as a numeric variable is not a good idea since some models might consider the numeric ordering or distances as something to learn. | Look up demographic variables based on zipcode. For example - With City Data you can look up income distribution, age ranges, etc. | Get latitude and longitude based on zip codes, which can be useful, especially for tree based models |",
            "url": "/notes/preprocessing/feature%20generation/datascience/kaggle/2019/06/14/week1.html",
            "relUrl": "/preprocessing/feature%20generation/datascience/kaggle/2019/06/14/week1.html",
            "date": " • Jun 14, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "/notes/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}