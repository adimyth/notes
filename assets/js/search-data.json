{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "/notes/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "How to win a data science competition (Week 3)",
            "content": "How to win a data science competition (Week 3) . Regression Metrics . . Mean Squared Error (MSE) . MSE is the most commonly used metric | Follows a parabolic curve | Optimal constant -&gt; Mean value of the target column | Error is always greater than 0 and 0 for a perfect model | Sensitive to outliers, as the values tend to be more biased towards the outliers | . Root Mean Squared Error (RMSE) . RMSE = sqrt(MSE) | Square root is taken to make the scale of the error same as the scale of the target | Every minimiser of MSE is also a minimiser of RMSE i.e. MSE(a) &gt; MSE(b) implies RMSE(a) &gt; RMSE(b) | We can minimise for MSE instead of RMSE; however this is not true for gradient based methods | . R-Squared . It’s difficult to judge whether our model is good or bad just by looking at the values of MSE &amp; RMSE | So, we should compare it against the baseline model which is mean for MSE &amp; RMSE When MSE is 0 ie the numerator is 0, then R-squared is 1 | When our model performs just like the baseline model or worse, then R-squared is 0 | . | To minimise R-squared we can still minimise MSE | . Mean Absolute Error (MAE) . MAE is less sensitive than MSE for outliers | Optimal constant -&gt; Median of the target column | If there are outliers use MAE, but if there are unexpected values that is useful use MSE | . Mean Absolute Percentage Error (MAPE) . . Mean Absolute Percentage Error (MAPE) is a weighted version of MAE | Optimal constant (MAPE) =&gt; Weighted median of the target values | MAPE is useful if you are interested in minimising the relative error rather than absolute error. For example, if the error between 9 &amp; 10 is much worse than error between 999 &amp; 1000, we can use this loss function | MAPE is undefined when the actual value is 0 | MAPE values can grow very large if the actual values (denominator) themselves are very low | MAPE is biased towards predictions that are lower than the actual values | MAPEs greater than 100% can occur. | . Example . yhat=20, yact=10, n=1 then MAPE = 50% . yhat=10, yact=20, n=1 then MAPE = 100% . Mean Squared Percentage Error (MSPE) . Mean Squared Percentage Error (MSPE) is a weighted version of MSE | As the target value for both of them increases, the curves flatten out | Optimal constant (MSPE) =&gt; Weighted mean of the target values | . Root Mean Square Logarithmic Error (RMSLE) . [RMSLE](https://hrngok.github.io/posts/metrics/#Root-Mean-Squared-Logaritmic-Error-(RMSLE) . RMSLE = RMSE(log(y_true+1), log(y_pred+1)) . from sklearn.metrics import mean_squared_log_error np.sqrt(mean_squared_log_error(y_true, y_pred)) . RMSE is the Root Mean Squared Error of the log-transformed predicted and log-transformed actual values. | Works for non-negative values only. A constant is added to the predictions and actual values in case the values are 0 as logarithm(0) is not defined | . When to Use? . Cares about relative error more than absolute error | We don’t want to penalize big differences when both the predicted and the actual are big numbers. | . Acutal=30, Predicted=40, RMSE=10, RMSLE=0.27 Actual=300, Predicted=400, RMSE=100, RMSLE=0.28 . We want to penalize under estimates more than over estimates | . Actual = 600, Predicted = 1000, RMSLE = 0.51 Actual = 1000, Predicted = 1400, RMSLE = 0.36 Sales &amp; Inventory products, where having extra supply might be more preferable to not being able to providing product as much as the demand . Poisson Loss . The Poisson loss is a loss function used for regression when modeling count data. Use the Poisson loss when you believe that the target value comes from a Poisson distribution and want to model the rate parameter conditioned on some input. Example - number of customers entering a shop, number of emails in a day, etc . Regression Loss . Huber Loss . Huber Loss is a combination of MAE &amp; MSE. It is quadratic for smaller errors and linear otherwise i.e it acts like MSE for error closer to 0 and like MAE elsewhere. It is differentiable at 0 unlike MAE. How small that error has to be to make it quadratic depends on a hyperparameter, 𝛿 (delta). It approaches MAE when 𝛿 ~ 0 and MSE when 𝛿 ~ ∞. . . . def huber(true, pred, delta): loss = np.where(np.abs(true-pred) &lt; delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2)) return np.sum(loss) . The downside being that we might need to train the hyper parameter 𝛿 (delta) which is an iterative process. . Log Cosh Loss . Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. . . It acts like a quadratic loss for smaller values of x and a shifted version of abs(x) for larger values of x. It is also twice differentiable, which is very useful for methods like XGBoost which use Hessian matrix for optimisation. . def logcosh(true, pred): loss = np.log(np.cosh(pred - true)) return np.sum(loss) . Classification Metrics . Soft Labels - Classifier’s probability scores | Hard Labels - Label associated with a prediction; usually argmax(soft_labels) | . Matthews Correlation Coefficient . Matthews Correlation Coefficient . . Used for binary classification | It is regarded as a balanced measure which can be used even if the classes are of very different sizes. | MCC is in essence a correlation coefficient between the observed and predicted binary classifications | A coefficient of +1 represents a perfect prediction (FP=FN=0), 0 no better than random prediction and −1(TP=TN=0) indicates total disagreement between prediction and observation. | MCC is also perfectly symmetric, so no class is more important than the other; if you switch the positive and negative, you’ll still get the same value. | . F1-Score . F1-Score . Precision . . Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive. | Precision is a good measure to use if the cost of false positive is high Eg. In a spam detector, if a non-spam mail is classified as spam, then the user might lose an important mail. | . Recall . . Recall actually calculates how many of the actual positives our model capture through labeling it as Positive | Recall is a good measure to use if the cost of false negative is high | . Accuracy . It is the fraction of correctly classified objects | The best constant for accuracy =&gt; Always predicting the class with highest frequency | Accuracy also doesn’t care how how confident the classifier is in the predictions i.e. it doesn’t care about the predicted class probabilities. It is harder to optimise and it only cares about the class labels. | It is most used when all the classes are equally important. | . Accuracy = (TP + TN) / (TP + TN + FP + FN) . LogLoss . LogLoss cares about soft labels i.e. class probabilities. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. | The probabilities always sum upto 1 . M - number of possible class labels (dog, cat, fish) | log - the natural logarithm | y - a binary indicator (0 or 1) of whether class label c is the correct classification for observation o | p - the model’s predicted probability that observation o is of class c | . | LogLoss hugely penalises wrong answers as can be seen in the above graph (actual class=1). As we move towards the correct prediction the loss decreases &amp; there’s an upward curve when we predict the class as 0 | Optimal Constant (LogLoss) =&gt; Set probabilities as frequency of the classes. [0.1, 0.9] if number of class1 samples is 10 and number of class2 samples is 90. | . AUC ROC . Tries all possible values as threshold &amp; then aggregates their scores. | Used only for binary tasks. Depends on the ordering of predictions, not on absolute values. | ROC is a probability curve and AUC represents degree or measure of separability. | Random prediction leads to AUC = 0.5 | . AUC ROC Curve . An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. | . Pairs Ordering . It is the ratio of correctly ordered pair upon total number of pairs. Ordering/Sorting of samples based on score is important. | AUC is the probability that score for the false positive will be greater than the score for true positive. | . AUC = # correctly ordered pairs / # total number of pairs . Example - Suppose the ordering based on probability score is [TP, FP, TP, TP, FP, FP] then there are 7 instances/pairs when score(FP) &gt; score(TP) out of 9 possible pairs. So, AUC = 7/9 TP is when a model has correctly classified a sample as positive class | FP is when a model has wrongly classified a sample as positive class | TN is when a model has correctly classified a sample as negative class | FN is when a model has wrongly classified a sample as negative class | . | . Quiz . How would multiplying all of the predictions from a given model by 2.0 (for example, if the model predicts 0.4, we multiply by 2.0 to get a prediction of 0.8) change the model’s performance as measured by AUC? . No change. AUC only cares about relative prediction scores. AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. This is clearly not the case for other metrics such as squared error, log loss, or prediction bias (discussed later). . LogLoss value for N number of classes with constant prediction. . log(N) . Cohen’s Kappa . The Kappa statistic (or value) is a metric that compares an Observed Accuracy with an Expected Accuracy (random chance). In addition, it takes into account random chance (agreement with a random classifier), which generally means it is less misleading than simply using accuracy as a metric. . The kappa statistic is often used as a measure of reliability between two human raters. In supervised learning, one of the raters reflects “ground truth”, and the other “rater” is the machine learning classifier. . Cohen’s Kappa = 1 - [(1 - accuracy) / (1 - baseline)] . Alternativey, since error=1 - accuracy, . Cohen’s Kappa = 1 - (error / baseline_error) . It can also, be written as - . Kappa = (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy) . Cohen’s Kappa ranges from [-1, 1] . Example . Assume Confusion Matrix for a binary classification task as follows .   Cats Dogs . Cats | 10 | 7 | . Dogs | 5 | 8 | . Observed Accuracy . It is simply the number of instances that were classified correctly throughout the entire confusion matrix ie the number of instances when the ground truth and classifier both agreed to a particular class for a sample. . Total Number of Instances = 30 . Observed Accuracy = (10+8 ) / 30 = 0.6 . Expected Accuracy . Marginal Frequency (Cats) = [(10+7) * (10+5)] / 30 = 8.5 . Marginal Frequency (Dogs) = [(7+8) * (5+8)] / 30 = 6.5 . Expected Accuracy = [Marginal Frequency (Cats) + Marginal Frequency (Dogs)] / 30 = 0.5 . Kappa . Kappa Score = (observed accuracy - expected accuracy)/(1 - expected accuracy) . Kappa Score = (0.60 - 0.50) / (1 - 0.50) = 0.20 . Quadratic Weighted Kappa . Quadratic Weighted Kappa . from sklearn.metrics import cohen_kappa_score, confusion_matrix qwk = cohen_kappa_score(actuals, preds, weights=&quot;quadratic&quot;) . Target &amp; Metric . Target Metric is what we want to optimise. It is how the model is eventually evaluated. | But no one really knows how to optimise target metrics efficiently. So, instead, we use Loss Functions, which is easy to optimise. Example it is not easy to optimise Accuracy Score, so we optimise for LogLoss &amp; eventually evaluate our model using Accuracy Score. | However, sometimes, the models can however optimise target metrics directly example MSE, LogLoss. | Sometimes it is not possible to optimise target metric directly but we can somehow preprocess the train data and use a model with a metric or loss function which is easy to optimise. For example, optimising MSPE or MAPE is not easy, but we can instead optimise MSE | Sometimes, we will optimise incorrect metrics but we will post-process to fit evaluation metric better | A technique that always works is early stopping. Suppose that we have to optimise for M2, but cannot optimise directly. So, we instead optimise for metric M1 and monitor metric M2 on validation set. We stop when the model starts overfitting on M2. | . Regression Metrics Optimization . MSE . Most of the libraries have MSE implemented as a loss function, so we can directly optimise for MSE. Synonyms - L2 Loss . Tree-Based :- LightGBM, RandomForest and XGBoost | Linear-Model :- SGDRegressor, Vowpal Vabbit | Neural Nets :- Pytorch, Keras, TF | . MAE . MAE is another commonly used metric, so most of the libraries can optimise for MAE directly. Synonyms - L1 Loss, Quantile loss, Huber loss . Tree-Based :- LightGBM, RandomForest, XGBoost | Neural Nets :- Pytorch, Keras, TF | . RMSLE . Train . Transform the target | . z = log(y+1) . Fit a model with MSE loss | . Test . Transform the prediction probabilities back | . y = exp(z) - 1 . Classification Metrics Optimization . LogLoss . Similar to MSE in terms of popularity &amp; hence implemented in almost all the major libraries. . Tree-Based :- LightGBM and XGBoost | Linear-Model :- SGDRegressor, Vowpal Vabbit | Neural Nets :- Pytorch, Keras, TF | . AUC . Optimise pairwise loss for optimising AUC . Tree-Based :- LightGBM and XGBoost | Neural Nets :- Pytorch, Keras, TF | . F1-Score . def f1_metric(y_true, y_pred): y_pred = K.round(y_pred) tp = K.sum(K.cast(y_true*y_pred, &#39;float&#39;), axis=0) tn = K.sum(K.cast((1-y_true)*(1-y_pred), &#39;float&#39;), axis=0) fp = K.sum(K.cast((1-y_true)*y_pred, &#39;float&#39;), axis=0) fn = K.sum(K.cast(y_true*(1-y_pred), &#39;float&#39;), axis=0) p = tp / (tp + fp + K.epsilon()) r = tp / (tp + fn + K.epsilon()) f1 = 2*p*r / (p+r+K.epsilon()) f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1) return K.mean(f1) def f1_loss(y_true, y_pred): tp = K.sum(K.cast(y_true*y_pred, &#39;float&#39;), axis=0) tn = K.sum(K.cast((1-y_true)*(1-y_pred), &#39;float&#39;), axis=0) fp = K.sum(K.cast((1-y_true)*y_pred, &#39;float&#39;), axis=0) fn = K.sum(K.cast(y_true*(1-y_pred), &#39;float&#39;), axis=0) p = tp / (tp + fp + K.epsilon()) r = tp / (tp + fn + K.epsilon()) f1 = 2*p*r / (p+r+K.epsilon()) f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1) return 1 - K.mean(f1) . Mean Encoding . Link 1 Link 2 . We encode each level of categorical variable with corresponding target mean. | The more complicated and non linear is the feature target dependency more effective is mean encoding | Greater the number of level in categorical features is a good indicator of using mean encodings | Prone to overfitting if the levels in categorical features in train and test datasets are different | Unlike other encoding techniques, mean encoding imposes an ordering | . Ways to construct mean encoding . Likelihood = #ones / (#ones + #zeros) . Weight of Evidence = log(#ones / #zeros) * 100 . Count = #ones . Difference = #ones - #zeros . Possible Leaks . Calculate means only on the train data. Then calculate mean encodings using by applying map function on both the train &amp; test data. . # col = categorical feature # target = target column means = X_train.groupby(col).target.mean() train[col + &#39;_mean_target&#39;] = train[col].map(means) val[col + &#39;_mean_target&#39;] = val[col].map(means) . Regularization . Expanding Mean . Introduces least amount of leakage | No hyper parameters to tune | Built in CatBoost | . cumsum = train.groupby(col)[&#39;target&#39;].cumsum() - train[&#39;target&#39;] cumcnt = train.groupby(col).cumcnt() train_new[col+&#39;_mean_target&#39;] = cumsum/cumcnt . Regression . Encode your categorical variable with the mean of the target. For every category, you calculate the corresponding mean of the target (among this category) and replace the value of a category with this mean. More flexible compared to classification tasks as you can use a variety of statistics like median, standard deviations, percentiles, etc. .",
            "url": "/notes/evaluation%20metrics/losses/datascience/kaggle/2019/06/26/week3.html",
            "relUrl": "/evaluation%20metrics/losses/datascience/kaggle/2019/06/26/week3.html",
            "date": " • Jun 26, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "How to win a data science competition (Week 2)",
            "content": "How to win a data science competition (Week 2) . 1. EDA . Better understand the data | Build an intuition about the data | Generate Hypothesis | Gain insights | . 1.1 Building intuition about data . Get domain knowledge Understand the data &amp; how people usually tackle the problem | Understand different features, how they relate with each other &amp; how does it affect the target feature | It helps to deeper understand the problem | . | Check if the data is intuitive/satisfactory And agrees with domain knowledge | Looking at quantiles is useful | . | Understand how the data was generated Crucial to setup proper validation | . | 1.2 Anonymised Data . 1.2.1 Explore Individual Features . Guess the meaning of the features | Guess the types of the column | . Play around with . pd.Series.value_counts pd.DataFrame.dtypes mean and standard deviation pd.DataFrame.sort_values(by=&quot;&quot;) np.diff . You never know what you might discover . 1.3 Visualization Tools . 1.3.1 Visualization of Individual Features . Misleading Histograms . plt.hist(x) # if degenerate graph; like all values pointing to 0 plt.hist(log(x)) # horizontal line implies repeating values &amp; the missing vertical lines implies well shuffled data plt.plot(x, &#39;.&#39;) df.describe() x.value_counts() x.isnull() . 1.3.2 Visualization of feature relations . Scatter Plots Can be used to check if the data distribution between train and test set are the same | . | Scatter Matrix | Correlation Matrix | 2. Data Cleaning . Concatenating train and test data first . 2.1 Remove constant columns . Remove features that have only 1 value for the entire feature. . traintest.nunique(axis==1) == 1 . Sometimes it might happen that there is constant value in the train data but multiple values for that feature in the test data. . Remove that feature | Train multiple models, each model for a different value in that feature | . 2.2 Remove duplicated columns . 2.2.1 Numerical . traintest.T.drop_duplicates() . 2.2.2 Categorical . # label encode each categorical feature as numeric &amp; repeat as above for f in categorical_feats: traintest[f] = traintest[f].factorize() traintest.T.drop_duplicates() . I usually look at the value_counts of each feature. If the feature has low cardinality, then category-wise count for 2 same features will also be the same. . 2.3 Others . Find duplicated rows and understand why they are duplicated | Check if the dataset is shuffled | . 3. Validation types . Holdout - Divide train into train and validation such that each sample belongs either to train or validation. There should be no repetition. In case, there are duplicated samples in the data, it might happen that one of the duplicated example is in val, this would lead to higher scores and improper hyperparameter selection. | . sklearn.model_selection.ShuffleSplit . K-Fold - K times slower than Holdout. Different than repeating Holdout K times in the sense, that it’s entirely possible that a sample may never be a part of validation set or might be a part of validation set multiple times. K-Fold however ensures that each sample is a part of validation set only once. | . sklearn.model_selection.Kfold . Leave-one-out Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset). | In the end you will get LOO predictions for every sample in the trainset and can calculate loss. | . | . Small amount of data Faster model to train . Stratification - Preserves same target distribution over different folds Small training data | Unbalanced datasets | Multiclass classification problem | . | . 4. Data splitting strategies . We should make train/validation split to mimic the train/test split. . Random/rowwise split | Time based split - Similar are moving window validation split | Id-based split | Combined split | . Quizes . Suppose we are given a huge dataset. We did a KFold validation once and noticed that scores on each fold are roughly the same. Which validation type is most practical to use? Use holdout validation scheme as the data is homogenous | . | If validation scores differ noticeably for each fold in KFold, we should stick with KFold in order to select statistically significant changes in scores while tuning a model. . | The features we generate depend on the train-test data splitting method. Is this true? True | . | Performance increase on a fixed cross-validation split guaranties performance increase on any cross-validation split. Incorrect. You can overfit to the specific CV-split. You should change your split from time to time to reduce the chance of overfitting. . | On Kaggle, make submissions which are Performing best on validation data - Assuming that train and test data distribution is same. | Performing best on public LB - Assuming that train and test data distribution is different. | . | Data Leaks . Basic Data Leaks . Row Based - Data may sometimes be shuffled by target variable. So simply adding row number might improve the score. | Id Based - Sometimes, ID may contain traces of information connected to target variable. | Meta-Data - Example image creation date, camera resolution etc in a classical cats vs dogs contest, where cats images were taken before dogs or taken using specific camera. | .",
            "url": "/notes/datascience/kaggle/eda/cross-validation/2019/06/19/week2.html",
            "relUrl": "/datascience/kaggle/eda/cross-validation/2019/06/19/week2.html",
            "date": " • Jun 19, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "How to win a data science competition (Week 1)",
            "content": "How to win a data science competition (Week 1) . Real World ML Pipeline . Understanding of business problem | Problem Formalization | Data Collection | Data Preprocessing | Modelling | Way to evaluate model in real life | Way to deploy model | . Data Science Competitions . Problem formalization (might be necessary sometimes) | Data Collection (possible in few competitions) | Data Preprocessing | Modelling | . Differences . Usually the hardest part of problem formalisation and evaluation metrics is already done. | Deployment is out of scope. | Model complexity, speed &amp; memory consumption doesn’t matter | . Resources . Random Forest | ExtraTrees Classifier | . 1. Review . 1.1 Linear Model . Especially good for sparse high dimensional data. Support Vector Machine (SVM) is a linear model with special loss function. Even with “kernel trick”, it’s still linear in new, extended space. Libraries: Scikit-learn and Vowpal Wabbit . 1.2 Tree Based Model . Decision trees used divide-and-conquer technique to recursively divide spaces into sub-spaces. Tree-based models can work very well with tabular data. . However, tree-based approaches cannot easily capture linear dependencies since it requires many splits. . . ExtraTrees classifier always tests random splits over fraction of features (in contrast to RandomForest, which tests all possible splits over fraction of features) . Libraries: Scikit-learn, XGBoost, LightGBM . 1.3 No Free Lunch Theorem . “There is no method which outperforms all others for all tasks“ . “For every method we can construct a task for which this particular method will not be the best“ . . SVMs have a linear boundaries | Decision trees have horizontal and vertical splits | Random Forest has many more of these horizontal and vertical axes and are relatively smoother | Naive Bayes has a smoother boundary compared to Neural Networks | . 2. Preprocessing . 2.1 Numeric Features . Preprocessing depends on the type of model we use. . Tree based models (Decision Trees) | Non-tree based models (KNNs, Neural Nets, Linear Models) | 2.1.1 Scaling . Non-tree based models are affected by the scale of the features. Different feature scaling results in different models quality. On the other hand, Decision Trees try to find the best split for a feature, no matter the scale. . Explanation . Nearest Neighbours The scale of features impacts the distance between samples. | With different scaling of the features nearest neighbours for a selected object can be very different. | . | Linear Models / Neural Networks Amount of regularization applied to a feature depends on the feature’s scale. | Optimization methods can perform differently depending on relative scale of features. | . | Others KMeans, SVMs, LDA, PCA, etc | . | . MinMax Scaler - The distribution of the feature before and after scaling remains the same. The value range is 0 to 1. | X = (X — X.min) / (X.max — X.min) . sklearn.preprocessing.MinMaxScaler . Standard Scaler - We always get a standard normal distribution. | X = (X — X.mean) / X.std . sklearn.preprocessing.StandardScaler . 2.1.2 Winsorization . Treating outliers by clipping values between two ranges - upper bound and lower bound. Say 1 &amp; 99 percentile of the feature values. . 2.1.3 Rank Transformation . Sorts an array and changes their values to indices. Similar to binning. . Example: rank([-100, 1, 1e-5]) = [0, 1, 2] rank([1000, 1, 10]) = [2, 0, 1] . Linear Models, KNN and Neural Networks usually benefit from this transformation. . Use scipy.stats.rankdata to create ranks. For test data, either store the mapping of value ranges to indices or concatenate train &amp; test data before applying the rank transformation. . 2.1.4 Log Transformation . Helps all non-tree based models especially neural nets. . . # log transformation np.log(1 + x) . Skewness transformation | This transformation is non-linear and will move outliers relatively closer to other samples. Also, values near zero become more distinguishable. | 2.2 Categorical Features . Excellent article . 2.2.1 Label Encoding . Works well with tree based methods. Linear methods struggle to extract meaning out of label encoded features. Categories encoded with numbers that close are to each other (usually) are not more related then categories encoded with numbers that far away from each other. . Categorical features are ordinal in nature | When the number of categorical features in the dataset is huge | Methods - . Alphabetically Sorted [S, C, Q] -&gt; [2, 1, 3] | sklearn.preprocessing.LabelEncoder . Order of Appearance [S, C, Q] -&gt; [1, 2, 3] | Pandas.factorize . 2.2.2 Frequency Encoding . Encode categories on the frequency of their occurrence. Preserves information about value distribution. Example: For a given list [S, C, Q, S, S, C, Q, S, C, S] the feature column can be encoded by replacing [S, C, Q] -&gt; [0.5, 0.3, 0.2] . Can be useful for both tree based &amp; non-tree based models. . 2.2.3 One hot Encoding . Works best for non-tree based models. One-hot encoded features are already scaled as the maximum value is 1 and minimum value is 0. Hence, suitable for non-tree based models which suffer from scaling issues. . On each split, trees can only separate one category from the others. So greater the number of categories, greater will be the number of splits. . 2.2.4 Mean Target Encoding . Encoding categorical variables with a mean target value (and also other target statistics) is a popular method for working with high cardinality features, especially useful for tree-based models. Mean encodings let models converge faster. Useful when working with high cardinality categorical features. Easy to overfit. . means = df.groupby(&#39;X&#39;)[&#39;y&#39;].mean() df[&#39;X&#39;] = df[&#39;X&#39;].map(means) . It is useful to apply target encoding when the number of samples of each category type belonging to the target value equal to say 1 is reasonable i.e if y has values only 0 and 1 &amp; say for the category a belonging to x_0 doesn’t have any target value as 1, then it’s mean will be 0. . For continous target value, feature is replaced by average target value. Example - if profession=teacher is the categorical feature &amp; salary is target, then teacher is replaced by average salary of teachers in the training set . 2.2.5 Binary Encoding . Binary encoding for categorical variables, similar to onehot, but stores categories as binary bitstrings. . First the categories are encoded as ordinal, then those integers are converted into binary code, then the digits from that binary string are split into separate columns. This encodes the data in fewer dimensions that one-hot, but with some distortion of the distances. . Different categories may share some of the same features. . 3 Datetime . 3.1 Periodicity . Useful to capture repetitive patterns in data. We can add features like . Day number in week | Month | Season | Year | Second | Minute | Hour | . 3.2 Time passed since a particular event . Case 1 - . In this case, all the samples become comparable between each other on one time scale. Days passed since January 1, 2000. . Case 2 - . In this case, the date will depend on the sample we are calculating this for. For example - number of days since last holiday, number of days since last weekend, number of days to the next holiday, etc. . 3.3 Difference between Dates . Number of days between two events. Example - Subtracting end_date - start_date gives number of years loan amount was paid. . 4. Coordinates . 4.1 Distance based Features . Find out the most interesting points in the map say the best school/hospital in the town, a museum, etc and calculate distance of the samples to this point. Add them as a feature in the training and test dataset. . 4.2 Aggregated Statistics . Calculate aggregated statistics for objects surrounding areas such as the total number of flats in the vicinity which can then be interpreted as areas of popularity.’ . Or you could calculate average price flat grouped by say pin code, area which would indicate expensiveness. . 5. Missing Values . Missing values can be represented in any way not necessarily as NaN. Some examples are -1, 999, , empty string, NaN, etc. | Sometimes missing values can contain information by themselves. | . 5.1 Imputation . Use a value outside the range of the normal values for a variable. like -1 ,or -9999 etc. | Replace with a likelihood – e.g. something that relates to the target variable. | Replace with something which makes sense. For example - sometimes null may mean zero | You may consider removing rows with many null values | Try to predict missing values based on subsets of know values. For example - in time series data, where rows are not independent of each other, then a missing value can be replaced by say averaging values from within a window | . 5.2 Adding isNull column . For each of the numerical features we can add an isNull column representing whether a particular row for a column in discussion contains empty values (NaN, NaT, None). Useful specially for tree-based methods &amp; neural nets. The downside is that we double the number of columns. . 5.3 General . Some models like XGBoost and CatBoost can deal with missing values out-of-box. These models have special methods to treat them and a model’s quality can benefit from it | Do not fill NaNs before feature generation | Impute with a feature mean/median | Remove rows with missing values | Reconstruct the missing values | . 5.4 Issue while generating new feature from existing feature which has null values . Order should be - . Missing Value Imputation -&gt; Feature Generation . We should be very careful about replacing missing values before the feature generation. . 6. Feature Extraction in Text and Images . 6.1 Bag of Words . N-grams can help utilise local context around each word because n-grams encode sequences of words. | It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. | N-grams features are typically sparse. N-grams deal with counts of words occurrences, and not every word can be found in a document. | Bag of words usually produces longer vectors than Word2Vec. Number of features in Bag of words approach is usually equal to number of unique words, while number of features in w2v is restricted to a constant, like 300 or so. | Meaning of a value in BOW matrix is the number of a word’s occurrences in a document. Values in vectors cannot be interpreted. | Word2Vec and Bag of words give different results. We can combine them to get more variety of results. | 6.1.1 Countvectorizer . Count the occurence of each word in a given document. . Needs scaling at the end to be use in linear models, as most occurring words will have higher counts and assume more importance. This is overcome by tf-idf vectorizer. . sklearn.feature_extraction.text.CountVectorize . 6.1.2 Tf-idf vectorizer . TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. . MAKING DOCUMENTS/SENTENCES OF DIFFERENT LENGTHS MORE COMPARABLE . Term Frequency measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization. Term-frequency (TF) normalises sum of the column values to 1. . TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). . BOOST MORE IMPORTANT FEATURES . Normalizing feature by the inverse fraction of documents. In this case features corresponding to frequent words will be scaled down compared to features corresponding to rarer words. . IDF(t) = log(Total number of documents / Number of documents with term t in it). . IDF scales features inversely proportionally to a number of word occurrences over documents . 7. Geographic Data . 7.1 ZIP Codes . Letting zip code as a numeric variable is not a good idea since some models might consider the numeric ordering or distances as something to learn. | Look up demographic variables based on zipcode. For example - With City Data you can look up income distribution, age ranges, etc. | Get latitude and longitude based on zip codes, which can be useful, especially for tree based models |",
            "url": "/notes/preprocessing/feature%20generation/datascience/kaggle/2019/06/14/week1.html",
            "relUrl": "/preprocessing/feature%20generation/datascience/kaggle/2019/06/14/week1.html",
            "date": " • Jun 14, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "/notes/https:/adimyth.github.io/",
          "relUrl": "/https:/adimyth.github.io/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}