{
  
    
        "post0": {
            "title": "How to win a data science competition (Week 1)",
            "content": "How to win a data science competition (Week 1) . Real World ML Pipeline . Understanding of business problem | Problem Formalization | Data Collection | Data Preprocessing | Modelling | Way to evaluate model in real life | Way to deploy model | . Data Science Competitions . Problem formalization (might be necessary sometimes) | Data Collection (possible in few competitions) | Data Preprocessing | Modelling | . Differences . Usually the hardest part of problem formalisation and evaluation metrics is already done. | Deployment is out of scope. | Model complexity, speed &amp; memory consumption doesn’t matter | . Resources . Random Forest | ExtraTrees Classifier | . 1. Review . 1.1 Linear Model . Especially good for sparse high dimensional data. Support Vector Machine (SVM) is a linear model with special loss function. Even with “kernel trick”, it’s still linear in new, extended space. Libraries: Scikit-learn and Vowpal Wabbit . 1.2 Tree Based Model . Decision trees used divide-and-conquer technique to recursively divide spaces into sub-spaces. Tree-based models can work very well with tabular data. . However, tree-based approaches cannot easily capture linear dependencies since it requires many splits. . . ExtraTrees classifier always tests random splits over fraction of features (in contrast to RandomForest, which tests all possible splits over fraction of features) . Libraries: Scikit-learn, XGBoost, LightGBM . 1.3 No Free Lunch Theorem . “There is no method which outperforms all others for all tasks“ . “For every method we can construct a task for which this particular method will not be the best“ . . SVMs have a linear boundaries | Decision trees have horizontal and vertical splits | Random Forest has many more of these horizontal and vertical axes and are relatively smoother | Naive Bayes has a smoother boundary compared to Neural Networks | . 2. Preprocessing . 2.1 Numeric Features . Preprocessing depends on the type of model we use. . Tree based models (Decision Trees) | Non-tree based models (KNNs, Neural Nets, Linear Models) | 2.1.1 Scaling . Non-tree based models are affected by the scale of the features. Different feature scaling results in different models quality. On the other hand, Decision Trees try to find the best split for a feature, no matter the scale. . Explanation . Nearest Neighbours The scale of features impacts the distance between samples. | With different scaling of the features nearest neighbours for a selected object can be very different. | . | Linear Models / Neural Networks Amount of regularization applied to a feature depends on the feature’s scale. | Optimization methods can perform differently depending on relative scale of features. | . | Others KMeans, SVMs, LDA, PCA, etc | . | . MinMax Scaler - The distribution of the feature before and after scaling remains the same. The value range is 0 to 1. | X = (X — X.min) / (X.max — X.min) . sklearn.preprocessing.MinMaxScaler . Standard Scaler - We always get a standard normal distribution. | X = (X — X.mean) / X.std . sklearn.preprocessing.StandardScaler . 2.1.2 Winsorization . Treating outliers by clipping values between two ranges - upper bound and lower bound. Say 1 &amp; 99 percentile of the feature values. . 2.1.3 Rank Transformation . Sorts an array and changes their values to indices. Similar to binning. . Example: rank([-100, 1, 1e-5]) = [0, 1, 2] rank([1000, 1, 10]) = [2, 0, 1] . Linear Models, KNN and Neural Networks usually benefit from this transformation. . Use scipy.stats.rankdata to create ranks. For test data, either store the mapping of value ranges to indices or concatenate train &amp; test data before applying the rank transformation. . 2.1.4 Log Transformation . Helps all non-tree based models especially neural nets. . . # log transformation np.log(1 + x) . Skewness transformation | This transformation is non-linear and will move outliers relatively closer to other samples. Also, values near zero become more distinguishable. | 2.2 Categorical Features . Excellent article . 2.2.1 Label Encoding . Works well with tree based methods. Linear methods struggle to extract meaning out of label encoded features. Categories encoded with numbers that close are to each other (usually) are not more related then categories encoded with numbers that far away from each other. . Categorical features are ordinal in nature | When the number of categorical features in the dataset is huge | Methods - . Alphabetically Sorted [S, C, Q] -&gt; [2, 1, 3] | sklearn.preprocessing.LabelEncoder . Order of Appearance [S, C, Q] -&gt; [1, 2, 3] | Pandas.factorize . 2.2.2 Frequency Encoding . Encode categories on the frequency of their occurrence. Preserves information about value distribution. Example: For a given list [S, C, Q, S, S, C, Q, S, C, S] the feature column can be encoded by replacing [S, C, Q] -&gt; [0.5, 0.3, 0.2] . Can be useful for both tree based &amp; non-tree based models. . 2.2.3 One hot Encoding . Works best for non-tree based models. One-hot encoded features are already scaled as the maximum value is 1 and minimum value is 0. Hence, suitable for non-tree based models which suffer from scaling issues. . On each split, trees can only separate one category from the others. So greater the number of categories, greater will be the number of splits. . 2.2.4 Mean Target Encoding . Encoding categorical variables with a mean target value (and also other target statistics) is a popular method for working with high cardinality features, especially useful for tree-based models. Mean encodings let models converge faster. Useful when working with high cardinality categorical features. Easy to overfit. . means = df.groupby(&#39;X&#39;)[&#39;y&#39;].mean() df[&#39;X&#39;] = df[&#39;X&#39;].map(means) . It is useful to apply target encoding when the number of samples of each category type belonging to the target value equal to say 1 is reasonable i.e if y has values only 0 and 1 &amp; say for the category a belonging to x_0 doesn’t have any target value as 1, then it’s mean will be 0. . For continous target value, feature is replaced by average target value. Example - if profession=teacher is the categorical feature &amp; salary is target, then teacher is replaced by average salary of teachers in the training set . 2.2.5 Binary Encoding . Binary encoding for categorical variables, similar to onehot, but stores categories as binary bitstrings. . First the categories are encoded as ordinal, then those integers are converted into binary code, then the digits from that binary string are split into separate columns. This encodes the data in fewer dimensions that one-hot, but with some distortion of the distances. . Different categories may share some of the same features. . 3 Datetime . 3.1 Periodicity . Useful to capture repetitive patterns in data. We can add features like . Day number in week | Month | Season | Year | Second | Minute | Hour | . 3.2 Time passed since a particular event . Case 1 - . In this case, all the samples become comparable between each other on one time scale. Days passed since January 1, 2000. . Case 2 - . In this case, the date will depend on the sample we are calculating this for. For example - number of days since last holiday, number of days since last weekend, number of days to the next holiday, etc. . 3.3 Difference between Dates . Number of days between two events. Example - Subtracting end_date - start_date gives number of years loan amount was paid. . 4. Coordinates . 4.1 Distance based Features . Find out the most interesting points in the map say the best school/hospital in the town, a museum, etc and calculate distance of the samples to this point. Add them as a feature in the training and test dataset. . 4.2 Aggregated Statistics . Calculate aggregated statistics for objects surrounding areas such as the total number of flats in the vicinity which can then be interpreted as areas of popularity.’ . Or you could calculate average price flat grouped by say pin code, area which would indicate expensiveness. . 5. Missing Values . Missing values can be represented in any way not necessarily as NaN. Some examples are -1, 999, , empty string, NaN, etc. | Sometimes missing values can contain information by themselves. | . 5.1 Imputation . Use a value outside the range of the normal values for a variable. like -1 ,or -9999 etc. | Replace with a likelihood – e.g. something that relates to the target variable. | Replace with something which makes sense. For example - sometimes null may mean zero | You may consider removing rows with many null values | Try to predict missing values based on subsets of know values. For example - in time series data, where rows are not independent of each other, then a missing value can be replaced by say averaging values from within a window | . 5.2 Adding isNull column . For each of the numerical features we can add an isNull column representing whether a particular row for a column in discussion contains empty values (NaN, NaT, None). Useful specially for tree-based methods &amp; neural nets. The downside is that we double the number of columns. . 5.3 General . Some models like XGBoost and CatBoost can deal with missing values out-of-box. These models have special methods to treat them and a model’s quality can benefit from it | Do not fill NaNs before feature generation | Impute with a feature mean/median | Remove rows with missing values | Reconstruct the missing values | . 5.4 Issue while generating new feature from existing feature which has null values . Order should be - . Missing Value Imputation -&gt; Feature Generation . We should be very careful about replacing missing values before the feature generation. . 6. Feature Extraction in Text and Images . 6.1 Bag of Words . N-grams can help utilise local context around each word because n-grams encode sequences of words. | It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. | N-grams features are typically sparse. N-grams deal with counts of words occurrences, and not every word can be found in a document. | Bag of words usually produces longer vectors than Word2Vec. Number of features in Bag of words approach is usually equal to number of unique words, while number of features in w2v is restricted to a constant, like 300 or so. | Meaning of a value in BOW matrix is the number of a word’s occurrences in a document. Values in vectors cannot be interpreted. | Word2Vec and Bag of words give different results. We can combine them to get more variety of results. | 6.1.1 Countvectorizer . Count the occurence of each word in a given document. . Needs scaling at the end to be use in linear models, as most occurring words will have higher counts and assume more importance. This is overcome by tf-idf vectorizer. . sklearn.feature_extraction.text.CountVectorize . 6.1.2 Tf-idf vectorizer . TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. . MAKING DOCUMENTS/SENTENCES OF DIFFERENT LENGTHS MORE COMPARABLE . Term Frequency measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization. Term-frequency (TF) normalises sum of the column values to 1. . TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). . BOOST MORE IMPORTANT FEATURES . Normalizing feature by the inverse fraction of documents. In this case features corresponding to frequent words will be scaled down compared to features corresponding to rarer words. . IDF(t) = log(Total number of documents / Number of documents with term t in it). . IDF scales features inversely proportionally to a number of word occurrences over documents . 7. Geographic Data . 7.1 ZIP Codes . Letting zip code as a numeric variable is not a good idea since some models might consider the numeric ordering or distances as something to learn. | Look up demographic variables based on zipcode. For example - With City Data you can look up income distribution, age ranges, etc. | Get latitude and longitude based on zip codes, which can be useful, especially for tree based models |",
            "url": "https://adimyth.github.io/notes/markdown/datascience/kaggle/2020/06/14/week1.html",
            "relUrl": "/markdown/datascience/kaggle/2020/06/14/week1.html",
            "date": " • Jun 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://adimyth.github.io/notes/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://adimyth.github.io/notes/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://adimyth.github.io/notes/https:/adimyth.github.io/",
          "relUrl": "/https:/adimyth.github.io/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adimyth.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}